{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import word_tokenize, pos_tag, download\n",
    "\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#id</th>\n",
       "      <th>label</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>argument_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arg219237_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>jesus loves plastic water bottles, and you can...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arg219203_arg219206</td>\n",
       "      <td>a2</td>\n",
       "      <td>The American Water companies are Aquafina (Pep...</td>\n",
       "      <td>Americans spend billions on bottled water ever...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arg219225_arg219284</td>\n",
       "      <td>a1</td>\n",
       "      <td>Banning plastic bottled water would be a huge ...</td>\n",
       "      <td>God created water bottles for a reason. Becaus...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arg219216_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>The water bottles are a safe source of water a...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arg219207_arg219294</td>\n",
       "      <td>a1</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>If bottled water did not exist, more people wo...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   #id label  \\\n",
       "0  arg219237_arg219207    a2   \n",
       "1  arg219203_arg219206    a2   \n",
       "2  arg219225_arg219284    a1   \n",
       "3  arg219216_arg219207    a2   \n",
       "4  arg219207_arg219294    a1   \n",
       "\n",
       "                                                  a1  \\\n",
       "0  jesus loves plastic water bottles, and you can...   \n",
       "1  The American Water companies are Aquafina (Pep...   \n",
       "2  Banning plastic bottled water would be a huge ...   \n",
       "3  The water bottles are a safe source of water a...   \n",
       "4  Bottled water consumption has grown exponentia...   \n",
       "\n",
       "                                                  a2  \\\n",
       "0  Bottled water consumption has grown exponentia...   \n",
       "1  Americans spend billions on bottled water ever...   \n",
       "2  God created water bottles for a reason. Becaus...   \n",
       "3  Bottled water consumption has grown exponentia...   \n",
       "4  If bottled water did not exist, more people wo...   \n",
       "\n",
       "                                      argument_group  \n",
       "0  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "1  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "2  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "3  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "4  ban-plastic-water-bottles_no-bad-for-the-econo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_compare_data_1 = \"data/UKPConvArg1Strict-CSV/\"\n",
    "\n",
    "files = listdir(path_to_compare_data_1)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for f in files:\n",
    "    path = path_to_compare_data_1 + f\n",
    "    one_argument = pd.read_csv(path, sep=\"\\t\")\n",
    "    one_argument[\"argument_group\"] = f\n",
    "    all_data.append(one_argument)\n",
    "    \n",
    "X_raw = pd.concat(all_data).reset_index(drop=True)\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a2\n",
       "1    a2\n",
       "2    a1\n",
       "3    a2\n",
       "4    a1\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw = X_raw.pop(\"label\")\n",
    "y_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset. Make y an integer and rename the arguments to use zero-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = (y_raw == \"a2\").astype(int)\n",
    "X = X_raw.drop([\"#id\"], axis=1)\n",
    "X.columns = [\"a0\", \"a1\", \"argument_group\"]\n",
    "\n",
    "def sentence_tags(text):\n",
    "#     text = text.decode('utf-8')\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    only_tags = [t[1] for t in tags]\n",
    "    return \" \".join(only_tags)\n",
    "\n",
    "X[\"a0_tags\"] = X.a0.apply(sentence_tags)\n",
    "X[\"a1_tags\"] = X.a1.apply(sentence_tags)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a benchmark model as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_words = set(['similarly', 'foremost', 'presumably', 'moreover', 'however', 'reason', 'otherwise', 'second,', 'still', 'first', 'even', 'ultimately', 'finally', 'therefore', 'addition', 'next', 'also', 'furthermore', 'conclusion', 'third', 'hand', 'another'])\n",
    "misspelled_words = set(['wouldn', 'dont', 'shouldn', 'india', 'firefox', 'didn', 'wouldnt', 'doesnt', 'thier', 'farquhar', 'couldn', 'commited', 'adam', 'theyre', 'sabrejimmy', 'persuit', 'definately', 'shouldnt', 'marrage', 'syona', 'alot', 'beleive', 'donot', 'commen', 'especialy', 'dicipline', 'arguement', 'likley', 'lously', 'havn', 'alcohlic', 'wasnt', 'aint', 'bcoz', 'hace', 'decir', 'planta', 'politian', 'acomplice', 'definetly', 'incestual', 'chasbas', 'soooo', 'coolio', 'hoolio', 'infront', 'creatoinism', 'enviroment', 'hasn', 'becasue', 'farquer', 'xada', 'atrever', 'menos', 'dathu', 'ihfej', 'humano', 'charector', 'blimem', 'shld', 'urself', 'tijkjdrk', 'sholud', 'jarman', 'responsibilily', 'preverted', 'plantas', 'aunque', 'sayin', 'xadgeno', 'imformation', 'xbanico', 'sfjiiytg', 'negleted', 'chupar', 'guil', 'xtklp', 'incharge', 'telvisions', 'telivision', 'igual', 'recursos', 'ushould', 'thik', 'supress', 'querido', 'idioma', 'unforms', 'unatrel', 'cauntious', 'evrybdy', 'bookxx', 'beleifs', 'eles', 'completly', 'isnot', 'beleave', 'beter', 'llaman', 'wdout', 'smoken', 'facr', 'illegaly', 'nowadayz', 'doont', 'somkers', 'somke', 'responsiblity', 'homosapien', 'dissappointed', 'criterium', 'hets', 'doughter', 'posible', 'strategizing', 'succeful', 'probaly', 'atleast', 'beileve', 'vida', 'pero', 'mench', 'playstations', 'niega', 'importhant', 'pensar', 'sentir', 'puede', 'aslong', 'ciggarettes', 'sooooooo', 'ebil', 'sito', 'botherd', 'diegnosedca', 'humanos', 'animales', 'suelen', 'aborto', 'matar', 'bullsh', 'employe', 'evryone', 'benifit', 'enviorment', 'lookin', 'persue', 'diffenrent', 'embroyo', 'undertsand', 'interveiw', 'becouse', 'afterschool', 'diferent', 'highschool', 'alreaddy', 'leagal', 'unpetty', 'themselfs', 'yoursel', 'defenceless', 'absolutley', 'peices', 'advencing', 'isnt', 'inequal', 'instinc', 'succesful', 'insctinc', 'disapointment', 'organisation', 'beemed', 'succeded', 'woulld', 'excuss', 'chil', 'singapura', 'majulah', 'mothernature', 'wannna', 'compulsaryy', 'preggo', 'weren', 'dieases', 'relize', 'coloured', 'actualy', 'expirience', 'itll', 'obecity', 'personhood', 'dosent', 'clases', 'mandortory', 'excersise', 'whloe', 'manditory', 'howzz', 'definatley', 'expirence', 'benifits', 'licence', 'echoworld', 'lieing', 'othr', 'alow', 'overal', 'theri', 'stoping', 'selfes', 'becoz', 'mmorning', 'mustn', 'espeacilly', 'perfomed', 'exersises', 'thankyou', 'dreamt', 'theirself', 'cuhz', 'learnt', 'malay', 'proble', 'wether', 'newscientist', 'evealution', 'makind', 'beleivers', 'argumentum', 'populum', 'extreamly', 'callad', 'beleives', 'scientologists', 'aquire', 'existance', 'addons', 'fanboy', 'realeased', 'wayyyyy', 'pointlessss', 'enititys', 'microsot', 'stylesheets', 'google', 'toolbar', 'phro', 'tohttp', 'evol', 'dinosauria', 'neccisary', 'varifiable', 'usgs', 'envirnment', 'nuff', 'polandspring', 'aspx', 'duboard', 'criters', 'worryz', 'excrament', 'produceing', 'evironment', 'poluted', 'healthywater', 'hypocryte', 'friendsjournal', 'garentee', 'compostable', 'youre', 'serval', 'comfortabley', 'suply', 'nikawater', 'nestlewaterscorporate', 'equis', 'ditrabutions', 'treehugger', 'extremly', 'weve', 'aynrandlexicon', 'flipppin', 'belivers', 'religon', 'biggots', 'athieists', 'besause', 'indepent', 'healp', 'lawl', 'sunday', 'spamming', 'therfore', 'recognise', 'simplier', 'didnt', 'xafsm', 'disputs', 'superbrain', 'politians', 'hitech', 'illitrate', 'literated', 'enought', 'specialy', 'fricken', 'opressed', 'illeteracy', 'toughy', 'somone', 'muder', 'marrie', 'sombody', 'accompalice', 'incase', 'hurst', 'basicly', 'preffer', 'nothong', 'tounges', 'contries', 'forgeting', 'ndians', 'hardwork', 'languags', 'utillised', 'prsns', 'ptential', 'manufactoring', 'dependant', 'alawys', 'violance', 'dissapointed', 'tought', 'figuer', 'msitake', 'arent', 'ooooooooh', 'sush', 'differnce', 'wats', 'aryabhatta', 'chatng', 'debatng', 'partical', 'pottential', 'nuissance', 'nalanda', 'jagah', 'achcha', 'hamara', 'britishers', 'orginal', 'americans', 'rama', 'krishna', 'vishvamitr', 'vishvguru', 'francisco', 'nutjobes', 'certainley', 'needn', 'roomates', 'marraige', 'secuality', 'respecful', 'harrassed', 'veiws', 'centry', 'commiting', 'beacuse', 'adware', 'nobrob', 'enuff', 'preinstall', 'derrrr', 'imho', 'weatherfox', 'apps', 'novanet', 'perfrom', 'popup', 'avaible', 'tooltip', 'spaking', 'saame', 'butthole', 'belifs', 'eachother', 'hackman', 'involed', 'throught', 'defence', 'worng', 'couldnt', 'reponsiblity', 'wong', 'woppen', 'nessasary', 'prenup', 'becuase', 'liklihood', 'couse', 'contriverse', 'accomodate', 'extrem', 'pepole', 'accomodations', 'sucied', 'wakoness', 'absoultly'])\n",
    "slang_words = set(['creep', 'jerk', 'basic', 'wicked', 'diss', 'props', 'unreal', 'dig', 'ripped', 'swole', 'wrecked', 'wasted', 'busted', 'awesome', 'trip', 'cool', 'chilling', 'chill', 'amped', 'blast', 'crush', 'dump', 'geek', 'sick', 'toasted', 'fail', 'epic', 'dunno', 'loser', 'rip', 'off', 'beat', 'bling', 'break', 'cheesy', 'cop', 'out', 'da', 'bomb', 'dope', 'downer', 'fab', 'flake', 'freak', 'disgusting', 'hooked', 'fleet', 'flawless', 'snatched', 'shorty', 'grill', 'hustle', 'grind', 'beef', 'fresh', 'word', 'wack', 'def', 'skeeze', 'ill', 'dough', 'mooch', 'boo', 'baller', 'bromance', 'dawg', 'dude', 'lol', 'ratchet', 'selfie', 'sweet', 'woke', 'neat', 'kidding', 'agame', 'bro', 'cash', 'cop', 'hip', 'jacked', 'hype', 'score', 'trash', 'riled', 'pissed', 'bummer', 'check', 'dead', 'totes'])\n",
    "important_parts_of_speech = [\"vbp\", \"vbp prp\", \"nn nn\", \"to\", \"dt\", \"dt nn\", \"cc\"]\n",
    "\n",
    "\n",
    "def n_general_transitions(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in transition_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_misspelled_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in misspelled_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_slang_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in slang_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "def percent_unique(x):\n",
    "    words = x.split()\n",
    "    unique = set(words)\n",
    "    percent_unique = len(unique)/len(words)\n",
    "    return percent_unique\n",
    "\n",
    "\n",
    "class TextBasedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        a0_raw = X.a0\n",
    "        a1_raw = X.a1\n",
    "        a0_simple = a0_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a1_simple = a1_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a0 = a0_simple.str\n",
    "        a1 = a1_simple.str\n",
    "        \n",
    "        # a0 features\n",
    "        X[\"characters_a0\"] = a0.len()\n",
    "        X[\"pronoun_counts_a0\"] = a0.count(\" you \") + a0.count(\" i \") + a0.count(\" me \") + a0.count(\" my \")\n",
    "        X[\"certain_lang_counts_a0\"] = a0.count(\" always \") + a0.count(\" never \") + a0.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a0\"] = a0.count(\" believe \") + a0.count(\" think \") + a0.count(\" feel \")\n",
    "        X[\"bold_counts_a0\"] = a0_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a0\"] = a0_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a0\"] = a0_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a0\"] = a0_raw.str.count(\",\")\n",
    "        X[\"words_a0\"] = a0.count(\" \") + 1\n",
    "        X[\"total_punctuation_a0\"] = a0_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a0\"] = X[\"total_punctuation_a0\"]/X[\"characters_a0\"]\n",
    "        X[\"end_of_sentence_punct_counts_a0\"] = a0_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a0\"] = (X[\"end_of_sentence_punct_counts_a0\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a0\"] = X[\"end_of_sentence_punct_counts_a0\"] + X[\"no_sentence_punctuation_a0\"]\n",
    "        X[\"average_sentence_word_len_a0\"] = X[\"words_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_sentence_char_len_a0\"] = X[\"characters_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_word_length_a0\"] = X[\"characters_a0\"]/X[\"words_a0\"]\n",
    "        X[\"n_transitions_a0\"] = a0_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a0\"] = a0_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a0\"] = a0_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a0\"] = a0_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a0\"] = a0_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a0\"] = a0_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a0\"] = a0_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a0\"] = a0_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a0\"] = a0_raw.str.count(\"http\")\n",
    "        X[\"vbp_a0\"] = X.a0_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a0\"] = X.a0_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a0\"] = X.a0_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a0\"] = X.a0_tags.str.count(\"TO\")\n",
    "        X[\"dt_a0\"] = X.a0_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a0\"] = X.a0_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a0\"] = X.a0_tags.str.count(\"CC\")\n",
    "        \n",
    "        # a1 features\n",
    "        X[\"characters_a1\"] = a1.len()\n",
    "        X[\"pronoun_counts_a1\"] = a1.count(\" you \") + a1.count(\" i \") + a1.count(\" me \") + a1.count(\" my \")\n",
    "        X[\"certain_lang_counts_a1\"] = a1.count(\" always \") + a1.count(\" never \") + a1.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a1\"] = a1.count(\" believe \") + a1.count(\" think \") + a1.count(\" feel \")\n",
    "        X[\"bold_counts_a1\"] = a1_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a1\"] = a1_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a1\"] = a1_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a1\"] = a1_raw.str.count(\",\")\n",
    "        X[\"words_a1\"] = a1.count(\" \") + 1\n",
    "        X[\"total_punctuation_a1\"] = a1_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a1\"] = X[\"total_punctuation_a1\"]/X[\"characters_a1\"]\n",
    "        X[\"end_of_sentence_punct_counts_a1\"] = a1_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a1\"] = (X[\"end_of_sentence_punct_counts_a1\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a1\"] = X[\"end_of_sentence_punct_counts_a1\"] + X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"average_sentence_word_len_a1\"] = X[\"words_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_char_len_a1\"] = X[\"characters_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_word_length_a1\"] = X[\"characters_a1\"]/X[\"words_a1\"]\n",
    "        X[\"n_transitions_a1\"] = a1_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a1\"] = a1_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a1\"] = a1_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a1\"] = a1_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a1\"] = a1_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a1\"] = a1_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a1\"] = a1_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a1\"] = a1_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a1\"] = a1_raw.str.count(\"http\")\n",
    "        X[\"vbp_a1\"] = X.a1_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a1\"] = X.a1_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a1\"] = X.a1_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a1\"] = X.a1_tags.str.count(\"TO\")\n",
    "        X[\"dt_a1\"] = X.a1_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a1\"] = X.a1_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a1\"] = X.a1_tags.str.count(\"CC\")\n",
    "        \n",
    "        # diff features\n",
    "        X[\"character_diff\"] = X[\"characters_a0\"] - X[\"characters_a1\"]\n",
    "        X[\"character_diff_percent\"] = X[\"character_diff\"]/X[\"characters_a1\"]\n",
    "        X[\"pronoun_count_diff\"] = X[\"pronoun_counts_a0\"] - X[\"pronoun_counts_a1\"]\n",
    "        X[\"certain_language_diff\"] = X[\"certain_lang_counts_a0\"] - X[\"certain_lang_counts_a1\"]\n",
    "        X[\"uncertain_language_diff\"] = X[\"uncertain_lang_counts_a0\"] - X[\"uncertain_lang_counts_a1\"]\n",
    "        X[\"bold_counts_diff\"] = X[\"bold_counts_a0\"] - X[\"bold_counts_a1\"]\n",
    "        X[\"because_counts_diff\"] = X[\"because_counts_a0\"] - X[\"because_counts_a1\"]\n",
    "        X[\"quote_counts_diff\"] = X[\"quote_counts_a0\"] - X[\"quote_counts_a1\"]\n",
    "        X[\"comma_counts_diff\"] = X[\"comma_counts_a0\"] - X[\"comma_counts_a1\"]\n",
    "        X[\"words_diff\"] = X[\"words_a0\"] - X[\"words_a1\"]\n",
    "        X[\"words_diff_percent\"] = X[\"words_diff\"]/X[\"words_a1\"]\n",
    "        X[\"punctuation_percent_diff\"] = X[\"punctuation_percent_a0\"] - X[\"punctuation_percent_a1\"]\n",
    "        X[\"punctuation_percent_diff_percent\"] = X[\"punctuation_percent_diff\"]/(X[\"punctuation_percent_a1\"] + 1)\n",
    "        X[\"no_sentence_punctuation_diff\"] = X[\"no_sentence_punctuation_a0\"] - X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"sentence_diff\"] = X[\"sentence_counts_a0\"] - X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_word_len_diff\"] = X[\"average_sentence_word_len_a0\"] - X[\"average_sentence_word_len_a1\"]\n",
    "        X[\"average_sentence_char_len_diff\"] = X[\"average_sentence_char_len_a0\"] - X[\"average_sentence_char_len_a1\"]\n",
    "        X[\"average_word_length_diff\"] = X[\"average_word_length_a0\"] - X[\"average_word_length_a1\"]\n",
    "        X[\"average_word_length_diff_percent\"] = X[\"average_word_length_diff\"]/X[\"average_word_length_a1\"]\n",
    "        X[\"n_transitions_diff\"] = X[\"n_transitions_a0\"] - X[\"n_transitions_a1\"]\n",
    "        X[\"n_misspelled_diff\"] = X[\"n_misspelled_a0\"] - X[\"n_misspelled_a1\"]\n",
    "        X[\"n_slang_diff\"] = X[\"n_slang_words_a0\"] - X[\"n_slang_words_a1\"]\n",
    "        X[\"percent_words_unique_diff\"] = X[\"percent_words_unique_a0\"] - X[\"percent_words_unique_a1\"]\n",
    "        X[\"first_word_cap_diff\"] = X[\"first_word_capitalized_a0\"] - X[\"first_word_capitalized_a1\"]\n",
    "        X[\"n_proper_nouns_diff\"] = X[\"n_proper_nouns_a0\"] - X[\"n_proper_nouns_a1\"]\n",
    "        X[\"all_caps_words_diff\"] = X[\"all_caps_words_a0\"] - X[\"all_caps_words_a1\"]\n",
    "        X[\"n_digits_diff\"] = X[\"n_digits_a0\"] - X[\"n_digits_a1\"]\n",
    "        X[\"n_links_diff\"] = X[\"n_links_a0\"] - X[\"n_links_a1\"]\n",
    "        X[\"vbp_diff\"] = X[\"vbp_a0\"] - X[\"vbp_a1\"]\n",
    "        X[\"vbp_prp_diff\"] = X[\"vbp_prp_a0\"] - X[\"vbp_prp_a1\"]\n",
    "        X[\"nn_nn_diff\"] = X[\"nn_nn_a0\"] - X[\"nn_nn_a1\"]\n",
    "        X[\"to_diff\"] = X[\"to_a0\"] - X[\"to_a1\"]\n",
    "        X[\"dt_diff\"] = X[\"dt_a0\"] - X[\"dt_a1\"]\n",
    "        X[\"dt_nn_diff\"] = X[\"dt_nn_a0\"] - X[\"dt_nn_a1\"]\n",
    "        X[\"cc_diff\"] = X[\"cc_a0\"] - X[\"cc_a1\"]       \n",
    "        return X\n",
    "\n",
    "    \n",
    "class KeepNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns = X.dtypes[X.dtypes != \"object\"].index.tolist()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.numeric_columns]\n",
    "\n",
    "    \n",
    "class OnlyDiffs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, on=True):\n",
    "        self.on = on\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.on:\n",
    "            diff_cols = [c for c in X.columns if \"diff\" in c]\n",
    "            other_cols = [\"a0_tags\", \"a1_tags\"]\n",
    "            return X[diff_cols+other_cols]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "\n",
    "class ColNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    \n",
    "def accuracy(y, y_hat, threshold=.5):\n",
    "    if (len(y_hat.shape) == 2) and (y_hat.shape[1] == 2):\n",
    "        y_hat = y_hat[:,1]\n",
    "    return accuracy_score(y, y_hat>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cv = LeaveOneGroupOut()\n",
    "group_cv.get_n_splits(X, y, X.argument_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps = [(\"simple_features\", TextBasedFeatures()),\n",
    "         (\"only_diff_columns\", OnlyDiffs(on=True)),\n",
    "         (\"keep_numeric_only\", KeepNumeric()),\n",
    "         (\"col_names\", ColNames())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_clean = pipe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798197424893\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_predictions = cross_val_predict(rf, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796480686695\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_clean, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792446351931\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = SVC(probability=True, cache_size=2000, random_state=42)\n",
    "\n",
    "svc_pipe = Pipeline([(\"scale\", scale), \n",
    "                     (\"model\", svc)])\n",
    "\n",
    "svc_predictions = cross_val_predict(svc_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, svc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795879828326\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_predictions = cross_val_predict(et, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797424892704\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params_from_gridsearch = {\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance',\n",
    "    'max_depth': 3,\n",
    "    'max_features': 1.0,\n",
    "    'subsample': 0.3}\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42, **best_params_from_gridsearch)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778798283262\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=35, n_jobs=-1)\n",
    "scale_knn = StandardScaler()\n",
    "\n",
    "knn_pipe = Pipeline([(\"scale\", scale_knn), \n",
    "                     (\"model\", knn)])\n",
    "\n",
    "knn_predictions = cross_val_predict(knn_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729785407725\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pt = Perceptron(random_state=42, n_jobs=-1)\n",
    "scale_pt = StandardScaler()\n",
    "\n",
    "pt_pipe = Pipeline([(\"scale\", scale_pt), \n",
    "                     (\"model\", pt)])\n",
    "\n",
    "pt_predictions = cross_val_predict(pt_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict')\n",
    "print(accuracy(y, pt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791416309013\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params = {\n",
    "    'alpha': 0.0001,\n",
    "    'beta_1': 0.94,\n",
    "    'hidden_layer_sizes': (10, 6),\n",
    "    'learning_rate_init': 0.002}\n",
    "\n",
    "mlp = MLPClassifier(random_state=42, **best_params)\n",
    "\n",
    "scale_mlp = StandardScaler()\n",
    "\n",
    "mlp_pipe = Pipeline([(\"scale\", scale_mlp), \n",
    "                     (\"model\", mlp)])\n",
    "\n",
    "mlp_predictions = cross_val_predict(mlp_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer one ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ens_1 = pd.DataFrame()\n",
    "X_ens_1[\"rf\"] = rf_predictions\n",
    "X_ens_1[\"lr\"] = lr_predictions\n",
    "X_ens_1[\"svc\"] = svc_predictions\n",
    "X_ens_1[\"et\"] = et_predictions\n",
    "X_ens_1[\"gb\"] = gb_predictions\n",
    "X_ens_1[\"knn\"] = knn_predictions\n",
    "X_ens_1[\"pt\"] = pt_predictions\n",
    "X_ens_1[\"mlp\"] = mlp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78669527897\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "\n",
    "lr_ens_1_predictions = cross_val_predict(lr, \n",
    "                                   X_ens_1, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798626609442\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_1_predictions = cross_val_predict(rf, X_ens_1, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798369098712\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_1_predictions = cross_val_predict(et, X_ens_1, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ens_1_detailed = pd.concat([X_ens_1, X_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79991416309\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_1_det_predictions = cross_val_predict(rf, X_ens_1_detailed, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799055793991\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_ens_1_det_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_ens_1_detailed, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800600858369\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_1_det_predictions = cross_val_predict(et, X_ens_1_detailed, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf2</th>\n",
       "      <th>et2</th>\n",
       "      <th>lr2</th>\n",
       "      <th>rf2d</th>\n",
       "      <th>et2d</th>\n",
       "      <th>lr2d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.787948</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.934257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.755912</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.965618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.322685</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.016381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.783276</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.912362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.345479</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.144167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rf2     et2       lr2   rf2d    et2d      lr2d\n",
       "0  0.968  0.9630  0.787948  0.979  0.9800  0.934257\n",
       "1  0.784  0.8540  0.755912  0.867  0.9015  0.965618\n",
       "2  0.044  0.0265  0.322685  0.050  0.0310  0.016381\n",
       "3  0.964  0.9930  0.783276  0.991  0.9840  0.912362\n",
       "4  0.056  0.0860  0.345479  0.031  0.0385  0.144167"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ens_2 = pd.DataFrame()\n",
    "X_ens_2[\"rf2\"] = rf_ens_1_predictions\n",
    "X_ens_2[\"et2\"] = et_ens_1_predictions\n",
    "X_ens_2[\"lr2\"] = lr_ens_1_predictions\n",
    "X_ens_2[\"rf2d\"] = rf_ens_1_det_predictions\n",
    "X_ens_2[\"et2d\"] = et_ens_1_det_predictions\n",
    "X_ens_2[\"lr2d\"] = lr_ens_1_det_predictions\n",
    "X_ens_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791502145923\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_2_predictions = cross_val_predict(rf, X_ens_2, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794248927039\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_2_predictions = cross_val_predict(et, X_ens_2, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803862660944\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.26867, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_pipe.fit(X_ens_2, y)\n",
    "\n",
    "\n",
    "lr_ens_2_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_ens_2, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sanchez\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Sanchez\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 0.26867275841806548}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=175,  subsample=.5,\n",
    "                                random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"loss\": [\"deviance\", \"exponential\"],\n",
    "    \"learning_rate\": [.045, .05, .05, .055, .06, .065, .07],\n",
    "    \"n_estimators\": [215, 220, 225, 225, 230, 235],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"subsample\": [.2, .3, .4, .5, .6, .7, .8, .9],\n",
    "    \"max_features\": [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0],\n",
    "    \n",
    "}\n",
    "\n",
    "grid = RandomizedSearchCV(gb, params, n_iter=50, scoring=\"accuracy\", n_jobs=-1, refit=False, cv=list(group_cv.split(X_clean, groups=X.argument_group)))\n",
    "\n",
    "grid.fit(X_clean, y)\n",
    "# gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "# print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.794335</td>\n",
       "      <td>[0.854166666667, 0.8975, 0.810397553517, 0.743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.793820</td>\n",
       "      <td>[0.888888888889, 0.89, 0.810397553517, 0.73180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.793820</td>\n",
       "      <td>[0.888888888889, 0.89, 0.810397553517, 0.73563...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.793734</td>\n",
       "      <td>[0.847222222222, 0.8975, 0.819571865443, 0.735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.793734</td>\n",
       "      <td>[0.881944444444, 0.89, 0.813455657492, 0.74712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.793391</td>\n",
       "      <td>[0.864583333333, 0.905, 0.82874617737, 0.74329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.793133</td>\n",
       "      <td>[0.850694444444, 0.9075, 0.816513761468, 0.747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.792189</td>\n",
       "      <td>[0.850694444444, 0.89, 0.819571865443, 0.70881...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.792103</td>\n",
       "      <td>[0.892361111111, 0.9025, 0.813455657492, 0.754...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.792017</td>\n",
       "      <td>[0.885416666667, 0.8825, 0.822629969419, 0.735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.791502</td>\n",
       "      <td>[0.864583333333, 0.9025, 0.837920489297, 0.750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.791416</td>\n",
       "      <td>[0.861111111111, 0.8775, 0.807339449541, 0.720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.790901</td>\n",
       "      <td>[0.854166666667, 0.9025, 0.804281345566, 0.720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.790815</td>\n",
       "      <td>[0.864583333333, 0.9025, 0.837920489297, 0.754...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.790386</td>\n",
       "      <td>[0.854166666667, 0.8825, 0.810397553517, 0.724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>[0.875, 0.8825, 0.804281345566, 0.724137931034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.790215</td>\n",
       "      <td>[0.871527777778, 0.89, 0.804281345566, 0.72030...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.790043</td>\n",
       "      <td>[0.84375, 0.89, 0.813455657492, 0.731800766284...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.789957</td>\n",
       "      <td>[0.84375, 0.89, 0.813455657492, 0.727969348659...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.789871</td>\n",
       "      <td>[0.864583333333, 0.9025, 0.807339449541, 0.750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.789785</td>\n",
       "      <td>[0.875, 0.8825, 0.80122324159, 0.727969348659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.789785</td>\n",
       "      <td>[0.857638888889, 0.89, 0.80122324159, 0.716475...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>[0.861111111111, 0.895, 0.804281345566, 0.7318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.789700</td>\n",
       "      <td>[0.875, 0.8825, 0.80122324159, 0.727969348659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.789528</td>\n",
       "      <td>[0.878472222222, 0.89, 0.816513761468, 0.75095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.789442</td>\n",
       "      <td>[0.868055555556, 0.9, 0.831804281346, 0.731800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.789356</td>\n",
       "      <td>[0.875, 0.8825, 0.80122324159, 0.727969348659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.788670</td>\n",
       "      <td>[0.864583333333, 0.9, 0.82874617737, 0.7471264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.788240</td>\n",
       "      <td>[0.850694444444, 0.885, 0.816513761468, 0.7049...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.788240</td>\n",
       "      <td>[0.881944444444, 0.905, 0.822629969419, 0.7471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.787983</td>\n",
       "      <td>[0.864583333333, 0.89, 0.819571865443, 0.71264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.787897</td>\n",
       "      <td>[0.854166666667, 0.8925, 0.804281345566, 0.720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.787639</td>\n",
       "      <td>[0.868055555556, 0.91, 0.792048929664, 0.73946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.787296</td>\n",
       "      <td>[0.864583333333, 0.905, 0.834862385321, 0.7432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>[0.864583333333, 0.905, 0.834862385321, 0.7432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.786867</td>\n",
       "      <td>[0.864583333333, 0.905, 0.822629969419, 0.7471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.786609</td>\n",
       "      <td>[0.868055555556, 0.9, 0.837920489297, 0.743295...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.786009</td>\n",
       "      <td>[0.857638888889, 0.91, 0.810397553517, 0.73946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.786009</td>\n",
       "      <td>[0.857638888889, 0.91, 0.810397553517, 0.73946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.785064</td>\n",
       "      <td>[0.854166666667, 0.91, 0.792048929664, 0.74329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.784034</td>\n",
       "      <td>[0.868055555556, 0.8925, 0.810397553517, 0.731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.782146</td>\n",
       "      <td>[0.857638888889, 0.895, 0.807339449541, 0.7394...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.781030</td>\n",
       "      <td>[0.864583333333, 0.895, 0.798165137615, 0.7432...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.781030</td>\n",
       "      <td>[0.864583333333, 0.9125, 0.810397553517, 0.750...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.667468</td>\n",
       "      <td>[0.864583333333, 0.9175, 0.804281345566, 0.521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.629013</td>\n",
       "      <td>[0.472222222222, 0.9175, 0.48623853211, 0.5210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate_init': 0.002, 'hidden_layer_si...</td>\n",
       "      <td>0.592961</td>\n",
       "      <td>[0.475694444444, 0.505, 0.798165137615, 0.5210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'learning_rate_init': 0.0015, 'hidden_layer_s...</td>\n",
       "      <td>0.573820</td>\n",
       "      <td>[0.475694444444, 0.51, 0.489296636086, 0.52490...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.547554</td>\n",
       "      <td>[0.861111111111, 0.5025, 0.483180428135, 0.517...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>{'learning_rate_init': 0.001, 'hidden_layer_si...</td>\n",
       "      <td>0.507983</td>\n",
       "      <td>[0.475694444444, 0.4925, 0.483180428135, 0.524...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           parameters  mean_validation_score  \\\n",
       "38  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.794335   \n",
       "43  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.793820   \n",
       "41  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.793820   \n",
       "4   {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.793734   \n",
       "31  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.793734   \n",
       "33  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.793391   \n",
       "48  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.793133   \n",
       "13  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.792189   \n",
       "9   {'learning_rate_init': 0.002, 'hidden_layer_si...               0.792103   \n",
       "10  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.792017   \n",
       "23  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.791502   \n",
       "24  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.791416   \n",
       "25  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.790901   \n",
       "35  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.790815   \n",
       "44  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.790386   \n",
       "29  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.790300   \n",
       "37  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.790215   \n",
       "20  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.790043   \n",
       "32  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.789957   \n",
       "21  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.789871   \n",
       "1   {'learning_rate_init': 0.001, 'hidden_layer_si...               0.789785   \n",
       "0   {'learning_rate_init': 0.002, 'hidden_layer_si...               0.789785   \n",
       "26  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.789700   \n",
       "6   {'learning_rate_init': 0.001, 'hidden_layer_si...               0.789700   \n",
       "17  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.789528   \n",
       "22  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.789442   \n",
       "46  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.789356   \n",
       "11  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.788670   \n",
       "12  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.788240   \n",
       "36  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.788240   \n",
       "28  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.787983   \n",
       "16  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.787897   \n",
       "49  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.787639   \n",
       "27  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.787296   \n",
       "2   {'learning_rate_init': 0.001, 'hidden_layer_si...               0.787124   \n",
       "19  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.786867   \n",
       "3   {'learning_rate_init': 0.001, 'hidden_layer_si...               0.786609   \n",
       "7   {'learning_rate_init': 0.001, 'hidden_layer_si...               0.786009   \n",
       "15  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.786009   \n",
       "42  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.785064   \n",
       "14  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.784034   \n",
       "47  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.782146   \n",
       "34  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.781030   \n",
       "5   {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.781030   \n",
       "30  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.667468   \n",
       "18  {'learning_rate_init': 0.002, 'hidden_layer_si...               0.629013   \n",
       "8   {'learning_rate_init': 0.002, 'hidden_layer_si...               0.592961   \n",
       "39  {'learning_rate_init': 0.0015, 'hidden_layer_s...               0.573820   \n",
       "40  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.547554   \n",
       "45  {'learning_rate_init': 0.001, 'hidden_layer_si...               0.507983   \n",
       "\n",
       "                                 cv_validation_scores  \n",
       "38  [0.854166666667, 0.8975, 0.810397553517, 0.743...  \n",
       "43  [0.888888888889, 0.89, 0.810397553517, 0.73180...  \n",
       "41  [0.888888888889, 0.89, 0.810397553517, 0.73563...  \n",
       "4   [0.847222222222, 0.8975, 0.819571865443, 0.735...  \n",
       "31  [0.881944444444, 0.89, 0.813455657492, 0.74712...  \n",
       "33  [0.864583333333, 0.905, 0.82874617737, 0.74329...  \n",
       "48  [0.850694444444, 0.9075, 0.816513761468, 0.747...  \n",
       "13  [0.850694444444, 0.89, 0.819571865443, 0.70881...  \n",
       "9   [0.892361111111, 0.9025, 0.813455657492, 0.754...  \n",
       "10  [0.885416666667, 0.8825, 0.822629969419, 0.735...  \n",
       "23  [0.864583333333, 0.9025, 0.837920489297, 0.750...  \n",
       "24  [0.861111111111, 0.8775, 0.807339449541, 0.720...  \n",
       "25  [0.854166666667, 0.9025, 0.804281345566, 0.720...  \n",
       "35  [0.864583333333, 0.9025, 0.837920489297, 0.754...  \n",
       "44  [0.854166666667, 0.8825, 0.810397553517, 0.724...  \n",
       "29  [0.875, 0.8825, 0.804281345566, 0.724137931034...  \n",
       "37  [0.871527777778, 0.89, 0.804281345566, 0.72030...  \n",
       "20  [0.84375, 0.89, 0.813455657492, 0.731800766284...  \n",
       "32  [0.84375, 0.89, 0.813455657492, 0.727969348659...  \n",
       "21  [0.864583333333, 0.9025, 0.807339449541, 0.750...  \n",
       "1   [0.875, 0.8825, 0.80122324159, 0.727969348659,...  \n",
       "0   [0.857638888889, 0.89, 0.80122324159, 0.716475...  \n",
       "26  [0.861111111111, 0.895, 0.804281345566, 0.7318...  \n",
       "6   [0.875, 0.8825, 0.80122324159, 0.727969348659,...  \n",
       "17  [0.878472222222, 0.89, 0.816513761468, 0.75095...  \n",
       "22  [0.868055555556, 0.9, 0.831804281346, 0.731800...  \n",
       "46  [0.875, 0.8825, 0.80122324159, 0.727969348659,...  \n",
       "11  [0.864583333333, 0.9, 0.82874617737, 0.7471264...  \n",
       "12  [0.850694444444, 0.885, 0.816513761468, 0.7049...  \n",
       "36  [0.881944444444, 0.905, 0.822629969419, 0.7471...  \n",
       "28  [0.864583333333, 0.89, 0.819571865443, 0.71264...  \n",
       "16  [0.854166666667, 0.8925, 0.804281345566, 0.720...  \n",
       "49  [0.868055555556, 0.91, 0.792048929664, 0.73946...  \n",
       "27  [0.864583333333, 0.905, 0.834862385321, 0.7432...  \n",
       "2   [0.864583333333, 0.905, 0.834862385321, 0.7432...  \n",
       "19  [0.864583333333, 0.905, 0.822629969419, 0.7471...  \n",
       "3   [0.868055555556, 0.9, 0.837920489297, 0.743295...  \n",
       "7   [0.857638888889, 0.91, 0.810397553517, 0.73946...  \n",
       "15  [0.857638888889, 0.91, 0.810397553517, 0.73946...  \n",
       "42  [0.854166666667, 0.91, 0.792048929664, 0.74329...  \n",
       "14  [0.868055555556, 0.8925, 0.810397553517, 0.731...  \n",
       "47  [0.857638888889, 0.895, 0.807339449541, 0.7394...  \n",
       "34  [0.864583333333, 0.895, 0.798165137615, 0.7432...  \n",
       "5   [0.864583333333, 0.9125, 0.810397553517, 0.750...  \n",
       "30  [0.864583333333, 0.9175, 0.804281345566, 0.521...  \n",
       "18  [0.472222222222, 0.9175, 0.48623853211, 0.5210...  \n",
       "8   [0.475694444444, 0.505, 0.798165137615, 0.5210...  \n",
       "39  [0.475694444444, 0.51, 0.489296636086, 0.52490...  \n",
       "40  [0.861111111111, 0.5025, 0.483180428135, 0.517...  \n",
       "45  [0.475694444444, 0.4925, 0.483180428135, 0.524...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.grid_scores_).sort_values([\"mean_validation_score\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(pd.DataFrame(grid.grid_scores_).parameters.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_results[\"scores\"] = pd.DataFrame(grid.grid_scores_).mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model__C</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.725691</td>\n",
       "      <td>0.803348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868358</td>\n",
       "      <td>0.803348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.214892</td>\n",
       "      <td>0.803348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.996037</td>\n",
       "      <td>0.803262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.834922</td>\n",
       "      <td>0.803262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model__C    scores\n",
       "0  2.725691  0.803348\n",
       "1  0.868358  0.803348\n",
       "2  2.214892  0.803348\n",
       "3  2.996037  0.803262\n",
       "4  2.834922  0.803262"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hidden_layer_sizes\n",
       "(10,)      0.790837\n",
       "(10, 3)    0.790721\n",
       "(10, 4)    0.791893\n",
       "(10, 5)    0.789264\n",
       "(10, 6)    0.791322\n",
       "Name: scores, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.groupby(\"hidden_layer_sizes\")[\"scores\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x222c6564e10>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nucf0UUX2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222c5cc42e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cv_results.model__C, cv_results.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta_1</th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.786524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.786609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.787124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.787382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.787639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.787983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.788326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.788498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.788584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.788670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.788755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.788841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.789013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.789528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.789700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.789785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.789957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.790215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.790386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.790472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.790558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.790558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.790730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.791159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.791159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 5)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.791330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.791416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.791674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.791760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.791845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.791931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.791931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.792103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.792103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.792446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.792618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.792704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.792876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.95</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.793047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10, 4)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.793047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.793219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.98</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.793391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.96</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.793562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.793734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.97</td>\n",
       "      <td>(10,)</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.793734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.94</td>\n",
       "      <td>(10, 6)</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.793906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha  beta_1 hidden_layer_sizes  learning_rate_init    scores\n",
       "37  0.0002    0.96            (10, 5)              0.0010  0.786524\n",
       "48  0.0001    0.95            (10, 5)              0.0010  0.786609\n",
       "4   0.0001    0.98            (10, 3)              0.0010  0.787124\n",
       "6   0.0001    0.98              (10,)              0.0020  0.787382\n",
       "8   0.0001    0.94            (10, 5)              0.0010  0.787639\n",
       "2   0.0010    0.97            (10, 5)              0.0010  0.787983\n",
       "27  0.0001    0.96            (10, 5)              0.0010  0.788326\n",
       "21  0.0010    0.95              (10,)              0.0015  0.788498\n",
       "43  0.0002    0.95            (10, 3)              0.0020  0.788584\n",
       "41  0.0010    0.94            (10, 6)              0.0015  0.788670\n",
       "11  0.0010    0.96            (10, 6)              0.0015  0.788755\n",
       "34  0.0010    0.98            (10, 6)              0.0020  0.788841\n",
       "33  0.0010    0.98            (10, 3)              0.0015  0.789013\n",
       "40  0.0002    0.98              (10,)              0.0015  0.789528\n",
       "42  0.0010    0.98            (10, 5)              0.0020  0.789700\n",
       "7   0.0020    0.96            (10, 5)              0.0020  0.789785\n",
       "10  0.0001    0.97            (10, 5)              0.0020  0.789957\n",
       "26  0.0010    0.97            (10, 3)              0.0020  0.790215\n",
       "36  0.0020    0.98            (10, 5)              0.0020  0.790300\n",
       "18  0.0001    0.94            (10, 4)              0.0010  0.790300\n",
       "1   0.0001    0.95            (10, 5)              0.0020  0.790386\n",
       "31  0.0001    0.96              (10,)              0.0015  0.790472\n",
       "0   0.0020    0.97            (10, 3)              0.0010  0.790558\n",
       "14  0.0020    0.95            (10, 6)              0.0020  0.790558\n",
       "15  0.0010    0.98            (10, 5)              0.0015  0.790730\n",
       "35  0.0010    0.97            (10, 3)              0.0010  0.790730\n",
       "39  0.0002    0.94            (10, 4)              0.0015  0.790730\n",
       "23  0.0002    0.96            (10, 5)              0.0015  0.791159\n",
       "32  0.0002    0.96            (10, 4)              0.0015  0.791159\n",
       "17  0.0002    0.95            (10, 5)              0.0015  0.791330\n",
       "20  0.0002    0.98              (10,)              0.0010  0.791416\n",
       "47  0.0002    0.94            (10, 6)              0.0020  0.791674\n",
       "22  0.0020    0.95            (10, 3)              0.0015  0.791760\n",
       "44  0.0020    0.97            (10, 4)              0.0010  0.791845\n",
       "5   0.0002    0.95            (10, 3)              0.0015  0.791931\n",
       "3   0.0002    0.96            (10, 4)              0.0020  0.791931\n",
       "25  0.0020    0.95            (10, 6)              0.0015  0.792103\n",
       "28  0.0020    0.97            (10, 4)              0.0015  0.792103\n",
       "29  0.0002    0.95              (10,)              0.0020  0.792446\n",
       "24  0.0020    0.98            (10, 6)              0.0010  0.792618\n",
       "38  0.0002    0.98            (10, 6)              0.0015  0.792704\n",
       "49  0.0001    0.96            (10, 4)              0.0010  0.792876\n",
       "19  0.0002    0.95            (10, 4)              0.0010  0.793047\n",
       "45  0.0020    0.97            (10, 4)              0.0020  0.793047\n",
       "30  0.0020    0.96              (10,)              0.0020  0.793219\n",
       "13  0.0020    0.98            (10, 6)              0.0015  0.793391\n",
       "9   0.0010    0.96            (10, 3)              0.0010  0.793562\n",
       "16  0.0001    0.94            (10, 3)              0.0015  0.793734\n",
       "46  0.0010    0.97              (10,)              0.0015  0.793734\n",
       "12  0.0001    0.94            (10, 6)              0.0020  0.793906"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.sort_values(\"scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80008583691\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=175,  subsample=.5,\n",
    "                                random_state=42)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796137339056\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=275,  subsample=.4,\n",
    "                                random_state=42)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795793991416\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=275,  subsample=.3,\n",
    "                                random_state=42)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=175, presort='auto', random_state=42,\n",
       "              subsample=0.66, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=175, subsample=.66,\n",
    "                                random_state=42)\n",
    "\n",
    "gb.fit(X_clean, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18d852927f0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18d85233fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(gb.oob_improvement_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799570815451\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=175,\n",
    "                                random_state=42)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.7988 +/- 0.11\n",
      "Scores: [ 0.86805556  0.91        0.81651376  0.70881226  0.86797753  0.67370892\n",
      "  0.83755274  0.83211679  0.79207921  0.84563758  0.85869565  0.80177515\n",
      "  0.71260997  0.79768786  0.77866667  0.76510067  0.80139373  0.78723404\n",
      "  0.81341108  0.88157895  0.78587699  0.80182232  0.74670185  0.69491525\n",
      "  0.69248826  0.81861575  0.81132075  0.81460674  0.77695167  0.79916318\n",
      "  0.81879195  0.84986595]\n"
     ]
    }
   ],
   "source": [
    "print(\"Average: %.4f +/- %.2f\" % (scores.mean(), scores.std()*1.96))\n",
    "print(\"Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "importances = pipe.named_steps[\"model\"].feature_importances_\n",
    "columns = pipe.named_steps[\"col_names\"].columns\n",
    "\n",
    "pd.Series(importances, index=columns).sort_values().plot(kind=\"barh\", figsize=(8,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = \" \".join(X.a0.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Banning plastic bottled water would be a huge mistake in this very moment. More than a million people in the United States purchase bottled water every day which is helping the economy com out of this recession we are in. maybe not in a big way but every kid of help counts! Bottled water also only makes less then 1&#xof; the worlds wastes and can be recycled! According to the National Association for PET Container resources, PET water bottles are no the most recycled container in curb side programs by weight and by number! <br/> http://www.nestlewaterscorporate.com/bottled_water_things_to_know/Video/index.html?@videoList.featured=31101711001'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = X.a0[2]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = X.a0_tags.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TFIDF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=100):\n",
    "        self.min_df = min_df\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        tags = X.a0_tags.unique()\n",
    "        self.tfidf = TfidfVectorizer(ngram_range=(1,15), \n",
    "                                     min_df=self.min_df)\n",
    "        self.tfidf.fit(tags)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        a0_vectors = self.tfidf.transform(X.a0_tags).todense()\n",
    "        a1_vectors = self.tfidf.transform(X.a1_tags).todense()\n",
    "        diffs = a0_vectors - a1_vectors\n",
    "        columns = [\"tf\"+str(i) for i in range(diffs.shape[1])]\n",
    "        vectors = pd.DataFrame(data=diffs, columns=columns, index=X.index)\n",
    "        new_data = pd.concat([X, vectors], axis=1)\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=100,\n",
       "        ngram_range=(1, 15), norm=u'l2', preprocessor=None,\n",
       "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1,15), \n",
    "                                  min_df=100)\n",
    "tf.fit(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a0_vectors = tf.transform(X.a0_tags).todense()\n",
    "a1_vectors = tf.transform(X.a1_tags).todense()\n",
    "diffs = a0_vectors - a1_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"tf\"+str(i) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11650, 199)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 50 ms, total: 19.8 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf = TFIDF().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'cc': 0,\n",
       " u'cc dt': 1,\n",
       " u'cc in': 2,\n",
       " u'cc jj': 3,\n",
       " u'cc nn': 4,\n",
       " u'cc prp': 5,\n",
       " u'cc rb': 6,\n",
       " u'cc vb': 7,\n",
       " u'cd': 8,\n",
       " u'dt': 9,\n",
       " u'dt jj': 10,\n",
       " u'dt jj nn': 11,\n",
       " u'dt jj nn in': 12,\n",
       " u'dt nn': 13,\n",
       " u'dt nn cc': 14,\n",
       " u'dt nn in': 15,\n",
       " u'dt nn in dt': 16,\n",
       " u'dt nn in nn': 17,\n",
       " u'dt nn nn': 18,\n",
       " u'dt nn prp': 19,\n",
       " u'dt nn to': 20,\n",
       " u'dt nn vbz': 21,\n",
       " u'dt nns': 22,\n",
       " u'dt vbz': 23,\n",
       " u'ex': 24,\n",
       " u'in': 25,\n",
       " u'in dt': 26,\n",
       " u'in dt jj': 27,\n",
       " u'in dt jj nn': 28,\n",
       " u'in dt nn': 29,\n",
       " u'in dt nn in': 30,\n",
       " u'in dt nns': 31,\n",
       " u'in in': 32,\n",
       " u'in jj': 33,\n",
       " u'in jj nn': 34,\n",
       " u'in nn': 35,\n",
       " u'in nn cc': 36,\n",
       " u'in nn in': 37,\n",
       " u'in nnp': 38,\n",
       " u'in nns': 39,\n",
       " u'in prp': 40,\n",
       " u'in prp md': 41,\n",
       " u'in prp nn': 42,\n",
       " u'in prp vbp': 43,\n",
       " u'in prp vbz': 44,\n",
       " u'in vbg': 45,\n",
       " u'jj': 46,\n",
       " u'jj cc': 47,\n",
       " u'jj in': 48,\n",
       " u'jj jj': 49,\n",
       " u'jj nn': 50,\n",
       " u'jj nn cc': 51,\n",
       " u'jj nn in': 52,\n",
       " u'jj nn nn': 53,\n",
       " u'jj nn prp': 54,\n",
       " u'jj nn vbz': 55,\n",
       " u'jj nnp': 56,\n",
       " u'jj nns': 57,\n",
       " u'jj nns in': 58,\n",
       " u'jj prp': 59,\n",
       " u'jj to': 60,\n",
       " u'jj to vb': 61,\n",
       " u'jjr': 62,\n",
       " u'jjr in': 63,\n",
       " u'jjs': 64,\n",
       " u'md': 65,\n",
       " u'md rb': 66,\n",
       " u'md rb vb': 67,\n",
       " u'md vb': 68,\n",
       " u'md vb dt': 69,\n",
       " u'md vb prp': 70,\n",
       " u'nn': 71,\n",
       " u'nn cc': 72,\n",
       " u'nn cc nn': 73,\n",
       " u'nn cc prp': 74,\n",
       " u'nn dt': 75,\n",
       " u'nn in': 76,\n",
       " u'nn in dt': 77,\n",
       " u'nn in dt nn': 78,\n",
       " u'nn in nn': 79,\n",
       " u'nn in prp': 80,\n",
       " u'nn jj': 81,\n",
       " u'nn md': 82,\n",
       " u'nn md vb': 83,\n",
       " u'nn nn': 84,\n",
       " u'nn nn in': 85,\n",
       " u'nn nnp': 86,\n",
       " u'nn nns': 87,\n",
       " u'nn prp': 88,\n",
       " u'nn prp md': 89,\n",
       " u'nn prp vbp': 90,\n",
       " u'nn rb': 91,\n",
       " u'nn to': 92,\n",
       " u'nn to vb': 93,\n",
       " u'nn vbd': 94,\n",
       " u'nn vbp': 95,\n",
       " u'nn vbz': 96,\n",
       " u'nn vbz rb': 97,\n",
       " u'nn wdt': 98,\n",
       " u'nnp': 99,\n",
       " u'nnp in': 100,\n",
       " u'nnp nn': 101,\n",
       " u'nnp nnp': 102,\n",
       " u'nnp vbz': 103,\n",
       " u'nns': 104,\n",
       " u'nns cc': 105,\n",
       " u'nns in': 106,\n",
       " u'nns in dt': 107,\n",
       " u'nns prp': 108,\n",
       " u'nns rb': 109,\n",
       " u'nns vbp': 110,\n",
       " u'prp': 111,\n",
       " u'prp in': 112,\n",
       " u'prp jj': 113,\n",
       " u'prp md': 114,\n",
       " u'prp md rb': 115,\n",
       " u'prp md rb vb': 116,\n",
       " u'prp md vb': 117,\n",
       " u'prp nn': 118,\n",
       " u'prp nns': 119,\n",
       " u'prp rb': 120,\n",
       " u'prp vb': 121,\n",
       " u'prp vbd': 122,\n",
       " u'prp vbp': 123,\n",
       " u'prp vbp dt': 124,\n",
       " u'prp vbp in': 125,\n",
       " u'prp vbp prp': 126,\n",
       " u'prp vbp rb': 127,\n",
       " u'prp vbp to': 128,\n",
       " u'prp vbz': 129,\n",
       " u'prp vbz rb': 130,\n",
       " u'rb': 131,\n",
       " u'rb dt': 132,\n",
       " u'rb in': 133,\n",
       " u'rb in prp': 134,\n",
       " u'rb jj': 135,\n",
       " u'rb prp': 136,\n",
       " u'rb rb': 137,\n",
       " u'rb vb': 138,\n",
       " u'rb vb dt': 139,\n",
       " u'rb vb prp': 140,\n",
       " u'rb vbg': 141,\n",
       " u'rb vbn': 142,\n",
       " u'rb vbp': 143,\n",
       " u'rb vbz': 144,\n",
       " u'rbr': 145,\n",
       " u'rp': 146,\n",
       " u'to': 147,\n",
       " u'to vb': 148,\n",
       " u'to vb dt': 149,\n",
       " u'to vb dt nn': 150,\n",
       " u'to vb in': 151,\n",
       " u'to vb jj': 152,\n",
       " u'to vb prp': 153,\n",
       " u'vb': 154,\n",
       " u'vb dt': 155,\n",
       " u'vb dt jj': 156,\n",
       " u'vb dt jj nn': 157,\n",
       " u'vb dt nn': 158,\n",
       " u'vb in': 159,\n",
       " u'vb in prp': 160,\n",
       " u'vb jj': 161,\n",
       " u'vb nn': 162,\n",
       " u'vb nns': 163,\n",
       " u'vb prp': 164,\n",
       " u'vb rb': 165,\n",
       " u'vb to': 166,\n",
       " u'vb to vb': 167,\n",
       " u'vb vbn': 168,\n",
       " u'vbd': 169,\n",
       " u'vbd dt': 170,\n",
       " u'vbg': 171,\n",
       " u'vbg dt': 172,\n",
       " u'vbg in': 173,\n",
       " u'vbg nn': 174,\n",
       " u'vbn': 175,\n",
       " u'vbn in': 176,\n",
       " u'vbp': 177,\n",
       " u'vbp dt': 178,\n",
       " u'vbp dt nn': 179,\n",
       " u'vbp in': 180,\n",
       " u'vbp jj': 181,\n",
       " u'vbp prp': 182,\n",
       " u'vbp rb': 183,\n",
       " u'vbp rb vb': 184,\n",
       " u'vbp to': 185,\n",
       " u'vbp to vb': 186,\n",
       " u'vbp vbg': 187,\n",
       " u'vbp vbn': 188,\n",
       " u'vbz': 189,\n",
       " u'vbz dt': 190,\n",
       " u'vbz dt nn': 191,\n",
       " u'vbz in': 192,\n",
       " u'vbz jj': 193,\n",
       " u'vbz prp': 194,\n",
       " u'vbz rb': 195,\n",
       " u'wdt': 196,\n",
       " u'wp': 197,\n",
       " u'wrb': 198}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
