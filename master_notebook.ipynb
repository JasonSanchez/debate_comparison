{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import word_tokenize, pos_tag, download\n",
    "\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#id</th>\n",
       "      <th>label</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>argument_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arg219237_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>jesus loves plastic water bottles, and you can...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arg219203_arg219206</td>\n",
       "      <td>a2</td>\n",
       "      <td>The American Water companies are Aquafina (Pep...</td>\n",
       "      <td>Americans spend billions on bottled water ever...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arg219225_arg219284</td>\n",
       "      <td>a1</td>\n",
       "      <td>Banning plastic bottled water would be a huge ...</td>\n",
       "      <td>God created water bottles for a reason. Becaus...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arg219216_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>The water bottles are a safe source of water a...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arg219207_arg219294</td>\n",
       "      <td>a1</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>If bottled water did not exist, more people wo...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   #id label  \\\n",
       "0  arg219237_arg219207    a2   \n",
       "1  arg219203_arg219206    a2   \n",
       "2  arg219225_arg219284    a1   \n",
       "3  arg219216_arg219207    a2   \n",
       "4  arg219207_arg219294    a1   \n",
       "\n",
       "                                                  a1  \\\n",
       "0  jesus loves plastic water bottles, and you can...   \n",
       "1  The American Water companies are Aquafina (Pep...   \n",
       "2  Banning plastic bottled water would be a huge ...   \n",
       "3  The water bottles are a safe source of water a...   \n",
       "4  Bottled water consumption has grown exponentia...   \n",
       "\n",
       "                                                  a2  \\\n",
       "0  Bottled water consumption has grown exponentia...   \n",
       "1  Americans spend billions on bottled water ever...   \n",
       "2  God created water bottles for a reason. Becaus...   \n",
       "3  Bottled water consumption has grown exponentia...   \n",
       "4  If bottled water did not exist, more people wo...   \n",
       "\n",
       "                                      argument_group  \n",
       "0  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "1  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "2  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "3  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "4  ban-plastic-water-bottles_no-bad-for-the-econo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_compare_data_1 = \"data/UKPConvArg1Strict-CSV/\"\n",
    "\n",
    "files = listdir(path_to_compare_data_1)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for f in files:\n",
    "    path = path_to_compare_data_1 + f\n",
    "    one_argument = pd.read_csv(path, sep=\"\\t\")\n",
    "    one_argument[\"argument_group\"] = f\n",
    "    all_data.append(one_argument)\n",
    "    \n",
    "X_raw = pd.concat(all_data).reset_index(drop=True)\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a2\n",
       "1    a2\n",
       "2    a1\n",
       "3    a2\n",
       "4    a1\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw = X_raw.pop(\"label\")\n",
    "y_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset. Make y an integer and rename the arguments to use zero-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = (y_raw == \"a2\").astype(int)\n",
    "X = X_raw.drop([\"#id\"], axis=1)\n",
    "X.columns = [\"a0\", \"a1\", \"argument_group\"]\n",
    "\n",
    "def sentence_tags(text):\n",
    "#     text = text.decode('utf-8')\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    only_tags = [t[1] for t in tags]\n",
    "    return \" \".join(only_tags)\n",
    "\n",
    "X[\"a0_tags\"] = X.a0.apply(sentence_tags)\n",
    "X[\"a1_tags\"] = X.a1.apply(sentence_tags)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a benchmark model as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_words = set(['similarly', 'foremost', 'presumably', 'moreover', 'however', 'reason', 'otherwise', 'second,', 'still', 'first', 'even', 'ultimately', 'finally', 'therefore', 'addition', 'next', 'also', 'furthermore', 'conclusion', 'third', 'hand', 'another'])\n",
    "misspelled_words = set(['wouldn', 'dont', 'shouldn', 'india', 'firefox', 'didn', 'wouldnt', 'doesnt', 'thier', 'farquhar', 'couldn', 'commited', 'adam', 'theyre', 'sabrejimmy', 'persuit', 'definately', 'shouldnt', 'marrage', 'syona', 'alot', 'beleive', 'donot', 'commen', 'especialy', 'dicipline', 'arguement', 'likley', 'lously', 'havn', 'alcohlic', 'wasnt', 'aint', 'bcoz', 'hace', 'decir', 'planta', 'politian', 'acomplice', 'definetly', 'incestual', 'chasbas', 'soooo', 'coolio', 'hoolio', 'infront', 'creatoinism', 'enviroment', 'hasn', 'becasue', 'farquer', 'xada', 'atrever', 'menos', 'dathu', 'ihfej', 'humano', 'charector', 'blimem', 'shld', 'urself', 'tijkjdrk', 'sholud', 'jarman', 'responsibilily', 'preverted', 'plantas', 'aunque', 'sayin', 'xadgeno', 'imformation', 'xbanico', 'sfjiiytg', 'negleted', 'chupar', 'guil', 'xtklp', 'incharge', 'telvisions', 'telivision', 'igual', 'recursos', 'ushould', 'thik', 'supress', 'querido', 'idioma', 'unforms', 'unatrel', 'cauntious', 'evrybdy', 'bookxx', 'beleifs', 'eles', 'completly', 'isnot', 'beleave', 'beter', 'llaman', 'wdout', 'smoken', 'facr', 'illegaly', 'nowadayz', 'doont', 'somkers', 'somke', 'responsiblity', 'homosapien', 'dissappointed', 'criterium', 'hets', 'doughter', 'posible', 'strategizing', 'succeful', 'probaly', 'atleast', 'beileve', 'vida', 'pero', 'mench', 'playstations', 'niega', 'importhant', 'pensar', 'sentir', 'puede', 'aslong', 'ciggarettes', 'sooooooo', 'ebil', 'sito', 'botherd', 'diegnosedca', 'humanos', 'animales', 'suelen', 'aborto', 'matar', 'bullsh', 'employe', 'evryone', 'benifit', 'enviorment', 'lookin', 'persue', 'diffenrent', 'embroyo', 'undertsand', 'interveiw', 'becouse', 'afterschool', 'diferent', 'highschool', 'alreaddy', 'leagal', 'unpetty', 'themselfs', 'yoursel', 'defenceless', 'absolutley', 'peices', 'advencing', 'isnt', 'inequal', 'instinc', 'succesful', 'insctinc', 'disapointment', 'organisation', 'beemed', 'succeded', 'woulld', 'excuss', 'chil', 'singapura', 'majulah', 'mothernature', 'wannna', 'compulsaryy', 'preggo', 'weren', 'dieases', 'relize', 'coloured', 'actualy', 'expirience', 'itll', 'obecity', 'personhood', 'dosent', 'clases', 'mandortory', 'excersise', 'whloe', 'manditory', 'howzz', 'definatley', 'expirence', 'benifits', 'licence', 'echoworld', 'lieing', 'othr', 'alow', 'overal', 'theri', 'stoping', 'selfes', 'becoz', 'mmorning', 'mustn', 'espeacilly', 'perfomed', 'exersises', 'thankyou', 'dreamt', 'theirself', 'cuhz', 'learnt', 'malay', 'proble', 'wether', 'newscientist', 'evealution', 'makind', 'beleivers', 'argumentum', 'populum', 'extreamly', 'callad', 'beleives', 'scientologists', 'aquire', 'existance', 'addons', 'fanboy', 'realeased', 'wayyyyy', 'pointlessss', 'enititys', 'microsot', 'stylesheets', 'google', 'toolbar', 'phro', 'tohttp', 'evol', 'dinosauria', 'neccisary', 'varifiable', 'usgs', 'envirnment', 'nuff', 'polandspring', 'aspx', 'duboard', 'criters', 'worryz', 'excrament', 'produceing', 'evironment', 'poluted', 'healthywater', 'hypocryte', 'friendsjournal', 'garentee', 'compostable', 'youre', 'serval', 'comfortabley', 'suply', 'nikawater', 'nestlewaterscorporate', 'equis', 'ditrabutions', 'treehugger', 'extremly', 'weve', 'aynrandlexicon', 'flipppin', 'belivers', 'religon', 'biggots', 'athieists', 'besause', 'indepent', 'healp', 'lawl', 'sunday', 'spamming', 'therfore', 'recognise', 'simplier', 'didnt', 'xafsm', 'disputs', 'superbrain', 'politians', 'hitech', 'illitrate', 'literated', 'enought', 'specialy', 'fricken', 'opressed', 'illeteracy', 'toughy', 'somone', 'muder', 'marrie', 'sombody', 'accompalice', 'incase', 'hurst', 'basicly', 'preffer', 'nothong', 'tounges', 'contries', 'forgeting', 'ndians', 'hardwork', 'languags', 'utillised', 'prsns', 'ptential', 'manufactoring', 'dependant', 'alawys', 'violance', 'dissapointed', 'tought', 'figuer', 'msitake', 'arent', 'ooooooooh', 'sush', 'differnce', 'wats', 'aryabhatta', 'chatng', 'debatng', 'partical', 'pottential', 'nuissance', 'nalanda', 'jagah', 'achcha', 'hamara', 'britishers', 'orginal', 'americans', 'rama', 'krishna', 'vishvamitr', 'vishvguru', 'francisco', 'nutjobes', 'certainley', 'needn', 'roomates', 'marraige', 'secuality', 'respecful', 'harrassed', 'veiws', 'centry', 'commiting', 'beacuse', 'adware', 'nobrob', 'enuff', 'preinstall', 'derrrr', 'imho', 'weatherfox', 'apps', 'novanet', 'perfrom', 'popup', 'avaible', 'tooltip', 'spaking', 'saame', 'butthole', 'belifs', 'eachother', 'hackman', 'involed', 'throught', 'defence', 'worng', 'couldnt', 'reponsiblity', 'wong', 'woppen', 'nessasary', 'prenup', 'becuase', 'liklihood', 'couse', 'contriverse', 'accomodate', 'extrem', 'pepole', 'accomodations', 'sucied', 'wakoness', 'absoultly'])\n",
    "slang_words = set(['creep', 'jerk', 'basic', 'wicked', 'diss', 'props', 'unreal', 'dig', 'ripped', 'swole', 'wrecked', 'wasted', 'busted', 'awesome', 'trip', 'cool', 'chilling', 'chill', 'amped', 'blast', 'crush', 'dump', 'geek', 'sick', 'toasted', 'fail', 'epic', 'dunno', 'loser', 'rip', 'off', 'beat', 'bling', 'break', 'cheesy', 'cop', 'out', 'da', 'bomb', 'dope', 'downer', 'fab', 'flake', 'freak', 'disgusting', 'hooked', 'fleet', 'flawless', 'snatched', 'shorty', 'grill', 'hustle', 'grind', 'beef', 'fresh', 'word', 'wack', 'def', 'skeeze', 'ill', 'dough', 'mooch', 'boo', 'baller', 'bromance', 'dawg', 'dude', 'lol', 'ratchet', 'selfie', 'sweet', 'woke', 'neat', 'kidding', 'agame', 'bro', 'cash', 'cop', 'hip', 'jacked', 'hype', 'score', 'trash', 'riled', 'pissed', 'bummer', 'check', 'dead', 'totes'])\n",
    "important_parts_of_speech = [\"vbp\", \"vbp prp\", \"nn nn\", \"to\", \"dt\", \"dt nn\", \"cc\"]\n",
    "\n",
    "\n",
    "def n_general_transitions(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in transition_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_misspelled_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in misspelled_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_slang_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in slang_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "def percent_unique(x):\n",
    "    words = x.split()\n",
    "    unique = set(words)\n",
    "    percent_unique = len(unique)/len(words)\n",
    "    return percent_unique\n",
    "\n",
    "\n",
    "class TextBasedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        a0_raw = X.a0\n",
    "        a1_raw = X.a1\n",
    "        a0_simple = a0_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a1_simple = a1_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a0 = a0_simple.str\n",
    "        a1 = a1_simple.str\n",
    "        \n",
    "        # a0 features\n",
    "        X[\"characters_a0\"] = a0.len()\n",
    "        X[\"pronoun_counts_a0\"] = a0.count(\" you \") + a0.count(\" i \") + a0.count(\" me \") + a0.count(\" my \")\n",
    "        X[\"certain_lang_counts_a0\"] = a0.count(\" always \") + a0.count(\" never \") + a0.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a0\"] = a0.count(\" believe \") + a0.count(\" think \") + a0.count(\" feel \")\n",
    "        X[\"bold_counts_a0\"] = a0_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a0\"] = a0_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a0\"] = a0_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a0\"] = a0_raw.str.count(\",\")\n",
    "        X[\"words_a0\"] = a0.count(\" \") + 1\n",
    "        X[\"total_punctuation_a0\"] = a0_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a0\"] = X[\"total_punctuation_a0\"]/X[\"characters_a0\"]\n",
    "        X[\"end_of_sentence_punct_counts_a0\"] = a0_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a0\"] = (X[\"end_of_sentence_punct_counts_a0\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a0\"] = X[\"end_of_sentence_punct_counts_a0\"] + X[\"no_sentence_punctuation_a0\"]\n",
    "        X[\"average_sentence_word_len_a0\"] = X[\"words_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_sentence_char_len_a0\"] = X[\"characters_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_word_length_a0\"] = X[\"characters_a0\"]/X[\"words_a0\"]\n",
    "        X[\"n_transitions_a0\"] = a0_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a0\"] = a0_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a0\"] = a0_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a0\"] = a0_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a0\"] = a0_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a0\"] = a0_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a0\"] = a0_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a0\"] = a0_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a0\"] = a0_raw.str.count(\"http\")\n",
    "        X[\"vbp_a0\"] = X.a0_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a0\"] = X.a0_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a0\"] = X.a0_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a0\"] = X.a0_tags.str.count(\"TO\")\n",
    "        X[\"dt_a0\"] = X.a0_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a0\"] = X.a0_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a0\"] = X.a0_tags.str.count(\"CC\")\n",
    "        \n",
    "        # a1 features\n",
    "        X[\"characters_a1\"] = a1.len()\n",
    "        X[\"pronoun_counts_a1\"] = a1.count(\" you \") + a1.count(\" i \") + a1.count(\" me \") + a1.count(\" my \")\n",
    "        X[\"certain_lang_counts_a1\"] = a1.count(\" always \") + a1.count(\" never \") + a1.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a1\"] = a1.count(\" believe \") + a1.count(\" think \") + a1.count(\" feel \")\n",
    "        X[\"bold_counts_a1\"] = a1_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a1\"] = a1_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a1\"] = a1_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a1\"] = a1_raw.str.count(\",\")\n",
    "        X[\"words_a1\"] = a1.count(\" \") + 1\n",
    "        X[\"total_punctuation_a1\"] = a1_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a1\"] = X[\"total_punctuation_a1\"]/X[\"characters_a1\"]\n",
    "        X[\"end_of_sentence_punct_counts_a1\"] = a1_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a1\"] = (X[\"end_of_sentence_punct_counts_a1\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a1\"] = X[\"end_of_sentence_punct_counts_a1\"] + X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"average_sentence_word_len_a1\"] = X[\"words_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_char_len_a1\"] = X[\"characters_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_word_length_a1\"] = X[\"characters_a1\"]/X[\"words_a1\"]\n",
    "        X[\"n_transitions_a1\"] = a1_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a1\"] = a1_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a1\"] = a1_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a1\"] = a1_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a1\"] = a1_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a1\"] = a1_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a1\"] = a1_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a1\"] = a1_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a1\"] = a1_raw.str.count(\"http\")\n",
    "        X[\"vbp_a1\"] = X.a1_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a1\"] = X.a1_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a1\"] = X.a1_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a1\"] = X.a1_tags.str.count(\"TO\")\n",
    "        X[\"dt_a1\"] = X.a1_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a1\"] = X.a1_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a1\"] = X.a1_tags.str.count(\"CC\")\n",
    "        \n",
    "        # diff features\n",
    "        X[\"character_diff\"] = X[\"characters_a0\"] - X[\"characters_a1\"]\n",
    "        X[\"character_diff_percent\"] = X[\"character_diff\"]/X[\"characters_a1\"]\n",
    "        X[\"pronoun_count_diff\"] = X[\"pronoun_counts_a0\"] - X[\"pronoun_counts_a1\"]\n",
    "        X[\"certain_language_diff\"] = X[\"certain_lang_counts_a0\"] - X[\"certain_lang_counts_a1\"]\n",
    "        X[\"uncertain_language_diff\"] = X[\"uncertain_lang_counts_a0\"] - X[\"uncertain_lang_counts_a1\"]\n",
    "        X[\"bold_counts_diff\"] = X[\"bold_counts_a0\"] - X[\"bold_counts_a1\"]\n",
    "        X[\"because_counts_diff\"] = X[\"because_counts_a0\"] - X[\"because_counts_a1\"]\n",
    "        X[\"quote_counts_diff\"] = X[\"quote_counts_a0\"] - X[\"quote_counts_a1\"]\n",
    "        X[\"comma_counts_diff\"] = X[\"comma_counts_a0\"] - X[\"comma_counts_a1\"]\n",
    "        X[\"words_diff\"] = X[\"words_a0\"] - X[\"words_a1\"]\n",
    "        X[\"words_diff_percent\"] = X[\"words_diff\"]/X[\"words_a1\"]\n",
    "        X[\"punctuation_percent_diff\"] = X[\"punctuation_percent_a0\"] - X[\"punctuation_percent_a1\"]\n",
    "        X[\"punctuation_percent_diff_percent\"] = X[\"punctuation_percent_diff\"]/(X[\"punctuation_percent_a1\"] + 1)\n",
    "        X[\"no_sentence_punctuation_diff\"] = X[\"no_sentence_punctuation_a0\"] - X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"sentence_diff\"] = X[\"sentence_counts_a0\"] - X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_word_len_diff\"] = X[\"average_sentence_word_len_a0\"] - X[\"average_sentence_word_len_a1\"]\n",
    "        X[\"average_sentence_char_len_diff\"] = X[\"average_sentence_char_len_a0\"] - X[\"average_sentence_char_len_a1\"]\n",
    "        X[\"average_word_length_diff\"] = X[\"average_word_length_a0\"] - X[\"average_word_length_a1\"]\n",
    "        X[\"average_word_length_diff_percent\"] = X[\"average_word_length_diff\"]/X[\"average_word_length_a1\"]\n",
    "        X[\"n_transitions_diff\"] = X[\"n_transitions_a0\"] - X[\"n_transitions_a1\"]\n",
    "        X[\"n_misspelled_diff\"] = X[\"n_misspelled_a0\"] - X[\"n_misspelled_a1\"]\n",
    "        X[\"n_slang_diff\"] = X[\"n_slang_words_a0\"] - X[\"n_slang_words_a1\"]\n",
    "        X[\"percent_words_unique_diff\"] = X[\"percent_words_unique_a0\"] - X[\"percent_words_unique_a1\"]\n",
    "        X[\"first_word_cap_diff\"] = X[\"first_word_capitalized_a0\"] - X[\"first_word_capitalized_a1\"]\n",
    "        X[\"n_proper_nouns_diff\"] = X[\"n_proper_nouns_a0\"] - X[\"n_proper_nouns_a1\"]\n",
    "        X[\"all_caps_words_diff\"] = X[\"all_caps_words_a0\"] - X[\"all_caps_words_a1\"]\n",
    "        X[\"n_digits_diff\"] = X[\"n_digits_a0\"] - X[\"n_digits_a1\"]\n",
    "        X[\"n_links_diff\"] = X[\"n_links_a0\"] - X[\"n_links_a1\"]\n",
    "        X[\"vbp_diff\"] = X[\"vbp_a0\"] - X[\"vbp_a1\"]\n",
    "        X[\"vbp_prp_diff\"] = X[\"vbp_prp_a0\"] - X[\"vbp_prp_a1\"]\n",
    "        X[\"nn_nn_diff\"] = X[\"nn_nn_a0\"] - X[\"nn_nn_a1\"]\n",
    "        X[\"to_diff\"] = X[\"to_a0\"] - X[\"to_a1\"]\n",
    "        X[\"dt_diff\"] = X[\"dt_a0\"] - X[\"dt_a1\"]\n",
    "        X[\"dt_nn_diff\"] = X[\"dt_nn_a0\"] - X[\"dt_nn_a1\"]\n",
    "        X[\"cc_diff\"] = X[\"cc_a0\"] - X[\"cc_a1\"]       \n",
    "        return X\n",
    "\n",
    "    \n",
    "class KeepNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns = X.dtypes[X.dtypes != \"object\"].index.tolist()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.numeric_columns]\n",
    "\n",
    "    \n",
    "class OnlyDiffs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, on=True):\n",
    "        self.on = on\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.on:\n",
    "            diff_cols = [c for c in X.columns if \"diff\" in c]\n",
    "            other_cols = [\"a0_tags\", \"a1_tags\"]\n",
    "            return X[diff_cols+other_cols]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "\n",
    "class ColNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    \n",
    "def accuracy(y, y_hat, threshold=.5):\n",
    "    if (len(y_hat.shape) == 2) and (y_hat.shape[1] == 2):\n",
    "        y_hat = y_hat[:,1]\n",
    "    return accuracy_score(y, y_hat>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cv = LeaveOneGroupOut()\n",
    "group_cv.get_n_splits(X, y, X.argument_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps = [(\"simple_features\", TextBasedFeatures()),\n",
    "         (\"only_diff_columns\", OnlyDiffs(on=True)),\n",
    "         (\"keep_numeric_only\", KeepNumeric()),\n",
    "         (\"col_names\", ColNames())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_clean = pipe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798197424893\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_predictions = cross_val_predict(rf, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796480686695\n",
      "Wall time: 5.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_clean, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792446351931\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = SVC(probability=True, cache_size=2000, random_state=42)\n",
    "\n",
    "svc_pipe = Pipeline([(\"scale\", scale), \n",
    "                     (\"model\", svc)])\n",
    "\n",
    "svc_predictions = cross_val_predict(svc_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, svc_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795879828326\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_predictions = cross_val_predict(et, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797424892704\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params_from_gridsearch = {\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance',\n",
    "    'max_depth': 3,\n",
    "    'max_features': 1.0,\n",
    "    'subsample': 0.3}\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42, **best_params_from_gridsearch)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778798283262\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=35, n_jobs=-1)\n",
    "scale_knn = StandardScaler()\n",
    "\n",
    "knn_pipe = Pipeline([(\"scale\", scale_knn), \n",
    "                     (\"model\", knn)])\n",
    "\n",
    "knn_predictions = cross_val_predict(knn_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729785407725\n",
      "Wall time: 5.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pt = Perceptron(random_state=42, n_jobs=-1)\n",
    "scale_pt = StandardScaler()\n",
    "\n",
    "pt_pipe = Pipeline([(\"scale\", scale_pt), \n",
    "                     (\"model\", pt)])\n",
    "\n",
    "pt_predictions = cross_val_predict(pt_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict')\n",
    "print(accuracy(y, pt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791416309013\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params = {\n",
    "    'alpha': 0.0001,\n",
    "    'beta_1': 0.94,\n",
    "    'hidden_layer_sizes': (10, 6),\n",
    "    'learning_rate_init': 0.002}\n",
    "\n",
    "mlp = MLPClassifier(random_state=42, **best_params)\n",
    "\n",
    "scale_mlp = StandardScaler()\n",
    "\n",
    "mlp_pipe = Pipeline([(\"scale\", scale_mlp), \n",
    "                     (\"model\", mlp)])\n",
    "\n",
    "mlp_predictions = cross_val_predict(mlp_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer one ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ens_1 = pd.DataFrame()\n",
    "X_ens_1[\"rf\"] = rf_predictions\n",
    "X_ens_1[\"lr\"] = lr_predictions\n",
    "X_ens_1[\"svc\"] = svc_predictions\n",
    "X_ens_1[\"et\"] = et_predictions\n",
    "X_ens_1[\"gb\"] = gb_predictions\n",
    "X_ens_1[\"knn\"] = knn_predictions\n",
    "X_ens_1[\"pt\"] = pt_predictions\n",
    "X_ens_1[\"mlp\"] = mlp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78669527897\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "\n",
    "lr_ens_1_predictions = cross_val_predict(lr, \n",
    "                                   X_ens_1, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798626609442\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_1_predictions = cross_val_predict(rf, X_ens_1, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798369098712\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_1_predictions = cross_val_predict(et, X_ens_1, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ens_1_detailed = pd.concat([X_ens_1, X_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79991416309\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_1_det_predictions = cross_val_predict(rf, X_ens_1_detailed, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799055793991\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_ens_1_det_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_ens_1_detailed, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800600858369\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_1_det_predictions = cross_val_predict(et, X_ens_1_detailed, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_1_det_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf2</th>\n",
       "      <th>et2</th>\n",
       "      <th>lr2</th>\n",
       "      <th>rf2d</th>\n",
       "      <th>et2d</th>\n",
       "      <th>lr2d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.787948</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>0.934257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.755912</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.965618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.322685</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.016381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.964</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.783276</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.912362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.0860</td>\n",
       "      <td>0.345479</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.144167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rf2     et2       lr2   rf2d    et2d      lr2d\n",
       "0  0.968  0.9630  0.787948  0.979  0.9800  0.934257\n",
       "1  0.784  0.8540  0.755912  0.867  0.9015  0.965618\n",
       "2  0.044  0.0265  0.322685  0.050  0.0310  0.016381\n",
       "3  0.964  0.9930  0.783276  0.991  0.9840  0.912362\n",
       "4  0.056  0.0860  0.345479  0.031  0.0385  0.144167"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ens_2 = pd.DataFrame()\n",
    "X_ens_2[\"rf2\"] = rf_ens_1_predictions\n",
    "X_ens_2[\"et2\"] = et_ens_1_predictions\n",
    "X_ens_2[\"lr2\"] = lr_ens_1_predictions\n",
    "X_ens_2[\"rf2d\"] = rf_ens_1_det_predictions\n",
    "X_ens_2[\"et2d\"] = et_ens_1_det_predictions\n",
    "X_ens_2[\"lr2d\"] = lr_ens_1_det_predictions\n",
    "X_ens_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791502145923\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_ens_2_predictions = cross_val_predict(rf, X_ens_2, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_ens_2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794248927039\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "et = ExtraTreesClassifier(n_estimators=2000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "et_ens_2_predictions = cross_val_predict(et, X_ens_2, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, et_ens_2_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803862660944\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.26867, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_pipe.fit(X_ens_2, y)\n",
    "\n",
    "\n",
    "lr_ens_2_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_ens_2, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_2_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
