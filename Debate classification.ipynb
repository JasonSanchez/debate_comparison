{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Argument Convincingness with Limited and Generic Features\n",
    "#### Jason Sanchez\n",
    "#### jason.sanchez@berkeley.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Determining which of two arguments is more convincing is a challenging task. Following previous research by Habernal and Gurevych (2016) and Chalaguine and Schulz (2017), this paper demonstrates how to classify which of two arguments is more convincing. \n",
    "\n",
    "Habernal and Gurevych trained a 64,000 feature SVC model and a bidirectional LSTM and achieved an accuracy of .78 and .76 respectively. Chalaguine and Schulz trained a model with only 35 features with a .77 accuracy. Their model relied on knowing the debate topics ahead of time and having a sizable example of arguments from each topic. \n",
    "\n",
    "This paper demonstrates how we can train a multilayer ensemble model to achieve a .803 accuracy. Furthermore, only 35 features are used and prior knowledge of the debate topics is not required.\n",
    "\n",
    "\n",
    "### Introduction and background\n",
    "\n",
    "The dataset consists of argument pairs. For example:\n",
    "\n",
    "|  |  |\n",
    "| --- | --- |\n",
    "| Argument 1 | Bottled water consumption has grown exponentially over the past ten to fifteen years. This growth has taken place globally, but particularly in Europe and North America. The bottled water industry has literally created its own water culture which is good for american industries. |\n",
    "| Argument 2 | If bottled water did not exist, more people would be drinking sweetened liquids becasue it would be the only portable drinks! People would become fat! |\n",
    "\n",
    "Groups of five human reviewers selected the argument they found most convincing among 11,650 such argument pairs among 32 different debate topics. See Habernal (2016) for full details of how the dataset was created.\n",
    "\n",
    "The established evaluation metric on this dataset is group cross-validation accuracy, where each group is one of the 32 debate topics. \"Accuracy\" in this paper refers to this metric.\n",
    "\n",
    "Habernal and Gurevych trained a 64,000 feature SVC model and a bidirectional LSTM and achieved an accuracy of .78 and .76 respectively. The LSTM was offered as a simpler alternative compared to the SVC with only a minor reduction in accuracy.\n",
    "\n",
    "Chalaguine and Schulz trained a model with only 35 features with a .77 accuracy. Their model relied on knowing the debate topics ahead of time and having a sizable example of arguments from each topic. For example, they compared the length of each argument to the average length of an argument in the same debate topic. These groups of features were the most performant features in their model. Without using these features, their model had a .6734 accuracy.\n",
    "\n",
    "This paper demonstrates how to achieve strong performance with thirty five debate agnostic features using ensemble techniques.\n",
    "\n",
    "We will investigate which features were important and final model architecture.\n",
    "\n",
    "\n",
    "### Methods\n",
    "\n",
    "#### Feature engineering\n",
    "Thirty five features were created. Every feature is generic which means none depend on prior knowledge of the debate topic. \n",
    "\n",
    "Here is a full description of which features were used and each feature's approximate relative importance. Importance was estimated using the mean decrease impurity from a Random Forest model trained on the entire dataset. Note, this was the only occurrence where a model was trained on the entire dataset at the same time; all other models were trained on cross-validation training folds. \n",
    "\n",
    "<table></table>\n",
    "\n",
    "Notice that several features had very small importances. These features could likely be excluded with no impact on final accuracy; however, they were included so future studies could get a baseline expectation of which features are useful.\n",
    "\n",
    "\n",
    "#### Model architecture\n",
    "Several model architectures were tested. Ultimately a simple model performed best of all model architectures. The chart below shows a high level picture of the model components and the accuracy of each component.\n",
    "\n",
    "<chart></chart>\n",
    "\n",
    "Each individual model's out of fold predictions were used as features in the final ensemble model. By removing one model at a time, we can see the marginal relative importance of each model to the final ensemble.\n",
    "\n",
    "<liftchart></liftchart>\n",
    "\n",
    "\n",
    "### Results and discussion\n",
    "* Current benchmarks\n",
    "* Per class performance\n",
    "* Model results and per class performance\n",
    "* Examples of incorrect predictions the model was most certain in and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aebd2c164939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# diff features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"character_diff\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"characters_a0\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"characters_a1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"character_diff_percent\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"character_diff\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"characters_a1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pronoun_count_diff\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pronoun_counts_a0\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pronoun_counts_a1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"certain_language_diff\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"certain_lang_counts_a0\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"certain_lang_counts_a1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "        # diff features\n",
    "        X[\"character_diff\"] = X[\"characters_a0\"] - X[\"characters_a1\"]\n",
    "        X[\"character_diff_percent\"] = X[\"character_diff\"]/X[\"characters_a1\"]\n",
    "        X[\"pronoun_count_diff\"] = X[\"pronoun_counts_a0\"] - X[\"pronoun_counts_a1\"]\n",
    "        X[\"certain_language_diff\"] = X[\"certain_lang_counts_a0\"] - X[\"certain_lang_counts_a1\"]\n",
    "        X[\"uncertain_language_diff\"] = X[\"uncertain_lang_counts_a0\"] - X[\"uncertain_lang_counts_a1\"]\n",
    "        X[\"bold_counts_diff\"] = X[\"bold_counts_a0\"] - X[\"bold_counts_a1\"]\n",
    "        X[\"because_counts_diff\"] = X[\"because_counts_a0\"] - X[\"because_counts_a1\"]\n",
    "        X[\"quote_counts_diff\"] = X[\"quote_counts_a0\"] - X[\"quote_counts_a1\"]\n",
    "        X[\"comma_counts_diff\"] = X[\"comma_counts_a0\"] - X[\"comma_counts_a1\"]\n",
    "        X[\"words_diff\"] = X[\"words_a0\"] - X[\"words_a1\"]\n",
    "        X[\"words_diff_percent\"] = X[\"words_diff\"]/X[\"words_a1\"]\n",
    "        X[\"punctuation_percent_diff\"] = X[\"punctuation_percent_a0\"] - X[\"punctuation_percent_a1\"]\n",
    "        X[\"punctuation_percent_diff_percent\"] = X[\"punctuation_percent_diff\"]/(X[\"punctuation_percent_a1\"] + 1)\n",
    "        X[\"no_sentence_punctuation_diff\"] = X[\"no_sentence_punctuation_a0\"] - X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"sentence_diff\"] = X[\"sentence_counts_a0\"] - X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_word_len_diff\"] = X[\"average_sentence_word_len_a0\"] - X[\"average_sentence_word_len_a1\"]\n",
    "        X[\"average_sentence_char_len_diff\"] = X[\"average_sentence_char_len_a0\"] - X[\"average_sentence_char_len_a1\"]\n",
    "        X[\"average_word_length_diff\"] = X[\"average_word_length_a0\"] - X[\"average_word_length_a1\"]\n",
    "        X[\"average_word_length_diff_percent\"] = X[\"average_word_length_diff\"]/X[\"average_word_length_a1\"]\n",
    "        X[\"n_transitions_diff\"] = X[\"n_transitions_a0\"] - X[\"n_transitions_a1\"]\n",
    "        X[\"n_misspelled_diff\"] = X[\"n_misspelled_a0\"] - X[\"n_misspelled_a1\"]\n",
    "        X[\"n_slang_diff\"] = X[\"n_slang_words_a0\"] - X[\"n_slang_words_a1\"]\n",
    "        X[\"percent_words_unique_diff\"] = X[\"percent_words_unique_a0\"] - X[\"percent_words_unique_a1\"]\n",
    "        X[\"first_word_cap_diff\"] = X[\"first_word_capitalized_a0\"] - X[\"first_word_capitalized_a1\"]\n",
    "        X[\"n_proper_nouns_diff\"] = X[\"n_proper_nouns_a0\"] - X[\"n_proper_nouns_a1\"]\n",
    "        X[\"all_caps_words_diff\"] = X[\"all_caps_words_a0\"] - X[\"all_caps_words_a1\"]\n",
    "        X[\"n_digits_diff\"] = X[\"n_digits_a0\"] - X[\"n_digits_a1\"]\n",
    "        X[\"n_links_diff\"] = X[\"n_links_a0\"] - X[\"n_links_a1\"]\n",
    "        X[\"vbp_diff\"] = X[\"vbp_a0\"] - X[\"vbp_a1\"]\n",
    "        X[\"vbp_prp_diff\"] = X[\"vbp_prp_a0\"] - X[\"vbp_prp_a1\"]\n",
    "        X[\"nn_nn_diff\"] = X[\"nn_nn_a0\"] - X[\"nn_nn_a1\"]\n",
    "        X[\"to_diff\"] = X[\"to_a0\"] - X[\"to_a1\"]\n",
    "        X[\"dt_diff\"] = X[\"dt_a0\"] - X[\"dt_a1\"]\n",
    "        X[\"dt_nn_diff\"] = X[\"dt_nn_a0\"] - X[\"dt_nn_a1\"]\n",
    "        X[\"cc_diff\"] = X[\"cc_a0\"] - X[\"cc_a1\"]       \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".504 accuracy = baseline score of always predicting the majority class\n",
    ".76 = Habernal and Gurevych Bidirectional LSTM\n",
    ".77 = Chalaguine and Schulz Feedforward Neural Network\n",
    ".78 = Habernal and Gurevych 64k feature SVC\n",
    ".804 = Sanchez Ensemble\n",
    ".935 accuracy = score of best human reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-da2a0bc39fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbenchmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.504\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m67.34\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.76\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.77\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.78\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.804\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.935\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Majority\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Simple NN - Generic\"\u001b[0m \u001b[1;34m\"LSTM\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Simple NN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SVC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Ensemble - Generic\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Best human\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbenchmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "benchmarks = pd.Series([.504, 67.34, .76, .77, .78, .804, .935], index=[\"Majority\", \"Simple NN - Generic\" \"LSTM\", \"Simple NN\", \"SVC\", \"Ensemble - Generic\", \"Best human\"])\n",
    "benchmarks.plot(kind=\"bar\")\n",
    "plt.ylim(.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-feb1bbfa3ea2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbenchmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m.504\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.76\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.77\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.78\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.804\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Majority\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LSTM\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Simple NN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SVC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Ensemble\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbenchmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.81\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "benchmarks = pd.Series([.504, .76, .77, .78, .804], index=[\"Majority\", \"LSTM\", \"Simple NN\", \"SVC\", \"Ensemble\"])\n",
    "benchmarks.plot(kind=\"bar\")\n",
    "plt.ylim(.5, .81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Argumentation analysis is important\n",
    "* Major ideas:\n",
    " * Problem is challenging because observations are not independent. Models that presume independence of obs will not generalize as well. Explain independence issue.\n",
    " * Generic features are critical to generalizable models\n",
    " * Ensembling is feature engineering\n",
    "\n",
    "\n",
    "### Background\n",
    "* Dataset description\n",
    "* Prior approaches\n",
    " * Habernal \n",
    " * Chalaguine\n",
    " \n",
    "\n",
    "### Methods\n",
    "* Features used and importances with RF\n",
    "* Chart of model architecture\n",
    "\n",
    "\n",
    "### Results and discussion\n",
    "* Current benchmarks\n",
    "* Per class performance\n",
    "* Model results and per class performance\n",
    "* Examples of incorrect predictions the model was most certain in and analysis\n",
    "* \n",
    "\n",
    "\n",
    "### Conclusion\n",
    "* Model is currently the best at this task\n",
    "* Distinction between knowing why an argument is better and being able to predict better arguments (i.e. longer = better)\n",
    "* Problem continues to be difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from nltk import word_tokenize, pos_tag, download\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#id</th>\n",
       "      <th>label</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>argument_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arg219237_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>jesus loves plastic water bottles, and you can...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arg219203_arg219206</td>\n",
       "      <td>a2</td>\n",
       "      <td>The American Water companies are Aquafina (Pep...</td>\n",
       "      <td>Americans spend billions on bottled water ever...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arg219225_arg219284</td>\n",
       "      <td>a1</td>\n",
       "      <td>Banning plastic bottled water would be a huge ...</td>\n",
       "      <td>God created water bottles for a reason. Becaus...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arg219216_arg219207</td>\n",
       "      <td>a2</td>\n",
       "      <td>The water bottles are a safe source of water a...</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arg219207_arg219294</td>\n",
       "      <td>a1</td>\n",
       "      <td>Bottled water consumption has grown exponentia...</td>\n",
       "      <td>If bottled water did not exist, more people wo...</td>\n",
       "      <td>ban-plastic-water-bottles_no-bad-for-the-econo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   #id label  \\\n",
       "0  arg219237_arg219207    a2   \n",
       "1  arg219203_arg219206    a2   \n",
       "2  arg219225_arg219284    a1   \n",
       "3  arg219216_arg219207    a2   \n",
       "4  arg219207_arg219294    a1   \n",
       "\n",
       "                                                  a1  \\\n",
       "0  jesus loves plastic water bottles, and you can...   \n",
       "1  The American Water companies are Aquafina (Pep...   \n",
       "2  Banning plastic bottled water would be a huge ...   \n",
       "3  The water bottles are a safe source of water a...   \n",
       "4  Bottled water consumption has grown exponentia...   \n",
       "\n",
       "                                                  a2  \\\n",
       "0  Bottled water consumption has grown exponentia...   \n",
       "1  Americans spend billions on bottled water ever...   \n",
       "2  God created water bottles for a reason. Becaus...   \n",
       "3  Bottled water consumption has grown exponentia...   \n",
       "4  If bottled water did not exist, more people wo...   \n",
       "\n",
       "                                      argument_group  \n",
       "0  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "1  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "2  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "3  ban-plastic-water-bottles_no-bad-for-the-econo...  \n",
       "4  ban-plastic-water-bottles_no-bad-for-the-econo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_compare_data_1 = \"data/UKPConvArg1Strict-CSV/\"\n",
    "\n",
    "files = listdir(path_to_compare_data_1)\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for f in files:\n",
    "    path = path_to_compare_data_1 + f\n",
    "    one_argument = pd.read_csv(path, sep=\"\\t\")\n",
    "    one_argument[\"argument_group\"] = f\n",
    "    all_data.append(one_argument)\n",
    "    \n",
    "X_raw = pd.concat(all_data).reset_index(drop=True)\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a2\n",
       "1    a2\n",
       "2    a1\n",
       "3    a2\n",
       "4    a1\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw = X_raw.pop(\"label\")\n",
    "y_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create base dataset. Make y an integer and rename the arguments to use zero-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = (y_raw == \"a2\").astype(int)\n",
    "X = X_raw.drop([\"#id\"], axis=1)\n",
    "X.columns = [\"a0\", \"a1\", \"argument_group\"]\n",
    "\n",
    "def sentence_tags(text):\n",
    "#     text = text.decode('utf-8')\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    only_tags = [t[1] for t in tags]\n",
    "    return \" \".join(only_tags)\n",
    "\n",
    "X[\"a0_tags\"] = X.a0.apply(sentence_tags)\n",
    "X[\"a1_tags\"] = X.a1.apply(sentence_tags)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: label, dtype: int32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a benchmark model as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_words = set(['similarly', 'foremost', 'presumably', 'moreover', 'however', 'reason', 'otherwise', 'second,', 'still', 'first', 'even', 'ultimately', 'finally', 'therefore', 'addition', 'next', 'also', 'furthermore', 'conclusion', 'third', 'hand', 'another'])\n",
    "misspelled_words = set(['wouldn', 'dont', 'shouldn', 'india', 'firefox', 'didn', 'wouldnt', 'doesnt', 'thier', 'farquhar', 'couldn', 'commited', 'adam', 'theyre', 'sabrejimmy', 'persuit', 'definately', 'shouldnt', 'marrage', 'syona', 'alot', 'beleive', 'donot', 'commen', 'especialy', 'dicipline', 'arguement', 'likley', 'lously', 'havn', 'alcohlic', 'wasnt', 'aint', 'bcoz', 'hace', 'decir', 'planta', 'politian', 'acomplice', 'definetly', 'incestual', 'chasbas', 'soooo', 'coolio', 'hoolio', 'infront', 'creatoinism', 'enviroment', 'hasn', 'becasue', 'farquer', 'xada', 'atrever', 'menos', 'dathu', 'ihfej', 'humano', 'charector', 'blimem', 'shld', 'urself', 'tijkjdrk', 'sholud', 'jarman', 'responsibilily', 'preverted', 'plantas', 'aunque', 'sayin', 'xadgeno', 'imformation', 'xbanico', 'sfjiiytg', 'negleted', 'chupar', 'guil', 'xtklp', 'incharge', 'telvisions', 'telivision', 'igual', 'recursos', 'ushould', 'thik', 'supress', 'querido', 'idioma', 'unforms', 'unatrel', 'cauntious', 'evrybdy', 'bookxx', 'beleifs', 'eles', 'completly', 'isnot', 'beleave', 'beter', 'llaman', 'wdout', 'smoken', 'facr', 'illegaly', 'nowadayz', 'doont', 'somkers', 'somke', 'responsiblity', 'homosapien', 'dissappointed', 'criterium', 'hets', 'doughter', 'posible', 'strategizing', 'succeful', 'probaly', 'atleast', 'beileve', 'vida', 'pero', 'mench', 'playstations', 'niega', 'importhant', 'pensar', 'sentir', 'puede', 'aslong', 'ciggarettes', 'sooooooo', 'ebil', 'sito', 'botherd', 'diegnosedca', 'humanos', 'animales', 'suelen', 'aborto', 'matar', 'bullsh', 'employe', 'evryone', 'benifit', 'enviorment', 'lookin', 'persue', 'diffenrent', 'embroyo', 'undertsand', 'interveiw', 'becouse', 'afterschool', 'diferent', 'highschool', 'alreaddy', 'leagal', 'unpetty', 'themselfs', 'yoursel', 'defenceless', 'absolutley', 'peices', 'advencing', 'isnt', 'inequal', 'instinc', 'succesful', 'insctinc', 'disapointment', 'organisation', 'beemed', 'succeded', 'woulld', 'excuss', 'chil', 'singapura', 'majulah', 'mothernature', 'wannna', 'compulsaryy', 'preggo', 'weren', 'dieases', 'relize', 'coloured', 'actualy', 'expirience', 'itll', 'obecity', 'personhood', 'dosent', 'clases', 'mandortory', 'excersise', 'whloe', 'manditory', 'howzz', 'definatley', 'expirence', 'benifits', 'licence', 'echoworld', 'lieing', 'othr', 'alow', 'overal', 'theri', 'stoping', 'selfes', 'becoz', 'mmorning', 'mustn', 'espeacilly', 'perfomed', 'exersises', 'thankyou', 'dreamt', 'theirself', 'cuhz', 'learnt', 'malay', 'proble', 'wether', 'newscientist', 'evealution', 'makind', 'beleivers', 'argumentum', 'populum', 'extreamly', 'callad', 'beleives', 'scientologists', 'aquire', 'existance', 'addons', 'fanboy', 'realeased', 'wayyyyy', 'pointlessss', 'enititys', 'microsot', 'stylesheets', 'google', 'toolbar', 'phro', 'tohttp', 'evol', 'dinosauria', 'neccisary', 'varifiable', 'usgs', 'envirnment', 'nuff', 'polandspring', 'aspx', 'duboard', 'criters', 'worryz', 'excrament', 'produceing', 'evironment', 'poluted', 'healthywater', 'hypocryte', 'friendsjournal', 'garentee', 'compostable', 'youre', 'serval', 'comfortabley', 'suply', 'nikawater', 'nestlewaterscorporate', 'equis', 'ditrabutions', 'treehugger', 'extremly', 'weve', 'aynrandlexicon', 'flipppin', 'belivers', 'religon', 'biggots', 'athieists', 'besause', 'indepent', 'healp', 'lawl', 'sunday', 'spamming', 'therfore', 'recognise', 'simplier', 'didnt', 'xafsm', 'disputs', 'superbrain', 'politians', 'hitech', 'illitrate', 'literated', 'enought', 'specialy', 'fricken', 'opressed', 'illeteracy', 'toughy', 'somone', 'muder', 'marrie', 'sombody', 'accompalice', 'incase', 'hurst', 'basicly', 'preffer', 'nothong', 'tounges', 'contries', 'forgeting', 'ndians', 'hardwork', 'languags', 'utillised', 'prsns', 'ptential', 'manufactoring', 'dependant', 'alawys', 'violance', 'dissapointed', 'tought', 'figuer', 'msitake', 'arent', 'ooooooooh', 'sush', 'differnce', 'wats', 'aryabhatta', 'chatng', 'debatng', 'partical', 'pottential', 'nuissance', 'nalanda', 'jagah', 'achcha', 'hamara', 'britishers', 'orginal', 'americans', 'rama', 'krishna', 'vishvamitr', 'vishvguru', 'francisco', 'nutjobes', 'certainley', 'needn', 'roomates', 'marraige', 'secuality', 'respecful', 'harrassed', 'veiws', 'centry', 'commiting', 'beacuse', 'adware', 'nobrob', 'enuff', 'preinstall', 'derrrr', 'imho', 'weatherfox', 'apps', 'novanet', 'perfrom', 'popup', 'avaible', 'tooltip', 'spaking', 'saame', 'butthole', 'belifs', 'eachother', 'hackman', 'involed', 'throught', 'defence', 'worng', 'couldnt', 'reponsiblity', 'wong', 'woppen', 'nessasary', 'prenup', 'becuase', 'liklihood', 'couse', 'contriverse', 'accomodate', 'extrem', 'pepole', 'accomodations', 'sucied', 'wakoness', 'absoultly'])\n",
    "slang_words = set(['creep', 'jerk', 'basic', 'wicked', 'diss', 'props', 'unreal', 'dig', 'ripped', 'swole', 'wrecked', 'wasted', 'busted', 'awesome', 'trip', 'cool', 'chilling', 'chill', 'amped', 'blast', 'crush', 'dump', 'geek', 'sick', 'toasted', 'fail', 'epic', 'dunno', 'loser', 'rip', 'off', 'beat', 'bling', 'break', 'cheesy', 'cop', 'out', 'da', 'bomb', 'dope', 'downer', 'fab', 'flake', 'freak', 'disgusting', 'hooked', 'fleet', 'flawless', 'snatched', 'shorty', 'grill', 'hustle', 'grind', 'beef', 'fresh', 'word', 'wack', 'def', 'skeeze', 'ill', 'dough', 'mooch', 'boo', 'baller', 'bromance', 'dawg', 'dude', 'lol', 'ratchet', 'selfie', 'sweet', 'woke', 'neat', 'kidding', 'agame', 'bro', 'cash', 'cop', 'hip', 'jacked', 'hype', 'score', 'trash', 'riled', 'pissed', 'bummer', 'check', 'dead', 'totes'])\n",
    "important_parts_of_speech = [\"vbp\", \"vbp prp\", \"nn nn\", \"to\", \"dt\", \"dt nn\", \"cc\"]\n",
    "\n",
    "\n",
    "def n_general_transitions(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in transition_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_misspelled_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in misspelled_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "\n",
    "def n_slang_words(x):\n",
    "    words = x.split()\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        if w in slang_words:\n",
    "            total += 1\n",
    "    return total\n",
    "\n",
    "def percent_unique(x):\n",
    "    words = x.split()\n",
    "    unique = set(words)\n",
    "    percent_unique = len(unique)/len(words)\n",
    "    return percent_unique\n",
    "\n",
    "\n",
    "class TextBasedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        a0_raw = X.a0\n",
    "        a1_raw = X.a1\n",
    "        a0_simple = a0_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a1_simple = a1_raw.str.lower().str.replace(\"[^a-zA-Z ]\", \"\")\n",
    "        a0 = a0_simple.str\n",
    "        a1 = a1_simple.str\n",
    "        \n",
    "        # a0 features\n",
    "        X[\"characters_a0\"] = a0.len()\n",
    "        X[\"pronoun_counts_a0\"] = a0.count(\" you \") + a0.count(\" i \") + a0.count(\" me \") + a0.count(\" my \")\n",
    "        X[\"certain_lang_counts_a0\"] = a0.count(\" always \") + a0.count(\" never \") + a0.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a0\"] = a0.count(\" believe \") + a0.count(\" think \") + a0.count(\" feel \")\n",
    "        X[\"bold_counts_a0\"] = a0_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a0\"] = a0_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a0\"] = a0_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a0\"] = a0_raw.str.count(\",\")\n",
    "        X[\"words_a0\"] = a0.count(\" \") + 1\n",
    "        X[\"total_punctuation_a0\"] = a0_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a0\"] = X[\"total_punctuation_a0\"]/X[\"characters_a0\"]\n",
    "        X[\"end_of_sentence_punct_counts_a0\"] = a0_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a0\"] = (X[\"end_of_sentence_punct_counts_a0\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a0\"] = X[\"end_of_sentence_punct_counts_a0\"] + X[\"no_sentence_punctuation_a0\"]\n",
    "        X[\"average_sentence_word_len_a0\"] = X[\"words_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_sentence_char_len_a0\"] = X[\"characters_a0\"]/X[\"sentence_counts_a0\"]\n",
    "        X[\"average_word_length_a0\"] = X[\"characters_a0\"]/X[\"words_a0\"]\n",
    "        X[\"n_transitions_a0\"] = a0_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a0\"] = a0_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a0\"] = a0_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a0\"] = a0_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a0\"] = a0_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a0\"] = a0_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a0\"] = a0_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a0\"] = a0_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a0\"] = a0_raw.str.count(\"http\")\n",
    "        X[\"vbp_a0\"] = X.a0_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a0\"] = X.a0_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a0\"] = X.a0_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a0\"] = X.a0_tags.str.count(\"TO\")\n",
    "        X[\"dt_a0\"] = X.a0_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a0\"] = X.a0_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a0\"] = X.a0_tags.str.count(\"CC\")\n",
    "        \n",
    "        # a1 features\n",
    "        X[\"characters_a1\"] = a1.len()\n",
    "        X[\"pronoun_counts_a1\"] = a1.count(\" you \") + a1.count(\" i \") + a1.count(\" me \") + a1.count(\" my \")\n",
    "        X[\"certain_lang_counts_a1\"] = a1.count(\" always \") + a1.count(\" never \") + a1.count(\" impossible \")\n",
    "        X[\"uncertain_lang_counts_a1\"] = a1.count(\" believe \") + a1.count(\" think \") + a1.count(\" feel \")\n",
    "        X[\"bold_counts_a1\"] = a1_raw.str.count(\"<br/>\")\n",
    "        X[\"because_counts_a1\"] = a1_raw.str.count(\"because\")\n",
    "        X[\"quote_counts_a1\"] = a1_raw.str.count(\"[\\\"']\")\n",
    "        X[\"comma_counts_a1\"] = a1_raw.str.count(\",\")\n",
    "        X[\"words_a1\"] = a1.count(\" \") + 1\n",
    "        X[\"total_punctuation_a1\"] = a1_raw.str.count(\"[.,?$!:;'\\\"-]\")\n",
    "        X[\"punctuation_percent_a1\"] = X[\"total_punctuation_a1\"]/X[\"characters_a1\"]\n",
    "        X[\"end_of_sentence_punct_counts_a1\"] = a1_raw.str.count(\"[!?.][ \\n]\")\n",
    "        X[\"no_sentence_punctuation_a1\"] = (X[\"end_of_sentence_punct_counts_a1\"]==0).astype(int)\n",
    "        X[\"sentence_counts_a1\"] = X[\"end_of_sentence_punct_counts_a1\"] + X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"average_sentence_word_len_a1\"] = X[\"words_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_char_len_a1\"] = X[\"characters_a1\"]/X[\"sentence_counts_a1\"]\n",
    "        X[\"average_word_length_a1\"] = X[\"characters_a1\"]/X[\"words_a1\"]\n",
    "        X[\"n_transitions_a1\"] = a1_simple.apply(n_general_transitions)\n",
    "        X[\"n_misspelled_a1\"] = a1_simple.apply(n_misspelled_words)\n",
    "        X[\"n_slang_words_a1\"] = a1_simple.apply(n_slang_words)\n",
    "        X[\"percent_words_unique_a1\"] = a1_simple.apply(percent_unique)\n",
    "        X[\"first_word_capitalized_a1\"] = a1_raw.apply(lambda x: x[0].isupper()*1)\n",
    "        X[\"n_proper_nouns_a1\"] = a1_raw.str.count(\"[^.!?;] [A-Z]\")\n",
    "        X[\"all_caps_words_a1\"] = a1_raw.str.count(\"[A-Z]+[ .!?]\")\n",
    "        X[\"n_digits_a1\"] = a1_raw.apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "        X[\"n_links_a1\"] = a1_raw.str.count(\"http\")\n",
    "        X[\"vbp_a1\"] = X.a1_tags.str.count(\"VBP\")\n",
    "        X[\"vbp_prp_a1\"] = X.a1_tags.str.count(\"VBP PRP\")\n",
    "        X[\"nn_nn_a1\"] = X.a1_tags.str.count(\"NN NN\")\n",
    "        X[\"to_a1\"] = X.a1_tags.str.count(\"TO\")\n",
    "        X[\"dt_a1\"] = X.a1_tags.str.count(\"DT\")\n",
    "        X[\"dt_nn_a1\"] = X.a1_tags.str.count(\"DT NN\")\n",
    "        X[\"cc_a1\"] = X.a1_tags.str.count(\"CC\")\n",
    "        \n",
    "        # diff features\n",
    "        X[\"character_diff\"] = X[\"characters_a0\"] - X[\"characters_a1\"]\n",
    "        X[\"character_diff_percent\"] = X[\"character_diff\"]/X[\"characters_a1\"]\n",
    "        X[\"pronoun_count_diff\"] = X[\"pronoun_counts_a0\"] - X[\"pronoun_counts_a1\"]\n",
    "        X[\"certain_language_diff\"] = X[\"certain_lang_counts_a0\"] - X[\"certain_lang_counts_a1\"]\n",
    "        X[\"uncertain_language_diff\"] = X[\"uncertain_lang_counts_a0\"] - X[\"uncertain_lang_counts_a1\"]\n",
    "        X[\"bold_counts_diff\"] = X[\"bold_counts_a0\"] - X[\"bold_counts_a1\"]\n",
    "        X[\"because_counts_diff\"] = X[\"because_counts_a0\"] - X[\"because_counts_a1\"]\n",
    "        X[\"quote_counts_diff\"] = X[\"quote_counts_a0\"] - X[\"quote_counts_a1\"]\n",
    "        X[\"comma_counts_diff\"] = X[\"comma_counts_a0\"] - X[\"comma_counts_a1\"]\n",
    "        X[\"words_diff\"] = X[\"words_a0\"] - X[\"words_a1\"]\n",
    "        X[\"words_diff_percent\"] = X[\"words_diff\"]/X[\"words_a1\"]\n",
    "        X[\"punctuation_percent_diff\"] = X[\"punctuation_percent_a0\"] - X[\"punctuation_percent_a1\"]\n",
    "        X[\"punctuation_percent_diff_percent\"] = X[\"punctuation_percent_diff\"]/(X[\"punctuation_percent_a1\"] + 1)\n",
    "        X[\"no_sentence_punctuation_diff\"] = X[\"no_sentence_punctuation_a0\"] - X[\"no_sentence_punctuation_a1\"]\n",
    "        X[\"sentence_diff\"] = X[\"sentence_counts_a0\"] - X[\"sentence_counts_a1\"]\n",
    "        X[\"average_sentence_word_len_diff\"] = X[\"average_sentence_word_len_a0\"] - X[\"average_sentence_word_len_a1\"]\n",
    "        X[\"average_sentence_char_len_diff\"] = X[\"average_sentence_char_len_a0\"] - X[\"average_sentence_char_len_a1\"]\n",
    "        X[\"average_word_length_diff\"] = X[\"average_word_length_a0\"] - X[\"average_word_length_a1\"]\n",
    "        X[\"average_word_length_diff_percent\"] = X[\"average_word_length_diff\"]/X[\"average_word_length_a1\"]\n",
    "        X[\"n_transitions_diff\"] = X[\"n_transitions_a0\"] - X[\"n_transitions_a1\"]\n",
    "        X[\"n_misspelled_diff\"] = X[\"n_misspelled_a0\"] - X[\"n_misspelled_a1\"]\n",
    "        X[\"n_slang_diff\"] = X[\"n_slang_words_a0\"] - X[\"n_slang_words_a1\"]\n",
    "        X[\"percent_words_unique_diff\"] = X[\"percent_words_unique_a0\"] - X[\"percent_words_unique_a1\"]\n",
    "        X[\"first_word_cap_diff\"] = X[\"first_word_capitalized_a0\"] - X[\"first_word_capitalized_a1\"]\n",
    "        X[\"n_proper_nouns_diff\"] = X[\"n_proper_nouns_a0\"] - X[\"n_proper_nouns_a1\"]\n",
    "        X[\"all_caps_words_diff\"] = X[\"all_caps_words_a0\"] - X[\"all_caps_words_a1\"]\n",
    "        X[\"n_digits_diff\"] = X[\"n_digits_a0\"] - X[\"n_digits_a1\"]\n",
    "        X[\"n_links_diff\"] = X[\"n_links_a0\"] - X[\"n_links_a1\"]\n",
    "        X[\"vbp_diff\"] = X[\"vbp_a0\"] - X[\"vbp_a1\"]\n",
    "        X[\"vbp_prp_diff\"] = X[\"vbp_prp_a0\"] - X[\"vbp_prp_a1\"]\n",
    "        X[\"nn_nn_diff\"] = X[\"nn_nn_a0\"] - X[\"nn_nn_a1\"]\n",
    "        X[\"to_diff\"] = X[\"to_a0\"] - X[\"to_a1\"]\n",
    "        X[\"dt_diff\"] = X[\"dt_a0\"] - X[\"dt_a1\"]\n",
    "        X[\"dt_nn_diff\"] = X[\"dt_nn_a0\"] - X[\"dt_nn_a1\"]\n",
    "        X[\"cc_diff\"] = X[\"cc_a0\"] - X[\"cc_a1\"]       \n",
    "        return X\n",
    "\n",
    "    \n",
    "class KeepNumeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns = X.dtypes[X.dtypes != \"object\"].index.tolist()\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.numeric_columns]\n",
    "\n",
    "    \n",
    "class OnlyDiffs(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, on=True):\n",
    "        self.on = on\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.on:\n",
    "            diff_cols = [c for c in X.columns if \"diff\" in c]\n",
    "            other_cols = [\"a0_tags\", \"a1_tags\"]\n",
    "            return X[diff_cols+other_cols]\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "\n",
    "class ColNames(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = X.columns\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    \n",
    "def accuracy(y, y_hat, threshold=.5):\n",
    "    if (len(y_hat.shape) == 2) and (y_hat.shape[1] == 2):\n",
    "        y_hat = y_hat[:,1]\n",
    "    return accuracy_score(y, y_hat>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_cv = LeaveOneGroupOut()\n",
    "group_cv.get_n_splits(X, y, X.argument_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "steps = [(\"simple_features\", TextBasedFeatures()),\n",
    "         (\"only_diff_columns\", OnlyDiffs(on=True)),\n",
    "         (\"keep_numeric_only\", KeepNumeric()),\n",
    "         (\"col_names\", ColNames())]\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_clean = pipe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798197424893\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)\n",
    "\n",
    "rf_predictions = cross_val_predict(rf, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796480686695\n",
      "Wall time: 5.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "scale = StandardScaler()\n",
    "lr = LogisticRegression(C=0.00125, random_state=42)\n",
    "lr_pipe = Pipeline([(\"scale\", scale), \n",
    "                    (\"model\", lr)])\n",
    "\n",
    "lr_predictions = cross_val_predict(lr_pipe, \n",
    "                                   X_clean, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797424892704\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params_from_gridsearch = {\n",
    "    'learning_rate': 0.05,\n",
    "    'loss': 'deviance',\n",
    "    'max_depth': 3,\n",
    "    'max_features': 1.0,\n",
    "    'subsample': 0.3}\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42, **best_params_from_gridsearch)\n",
    "\n",
    "gb_predictions = cross_val_predict(gb, X_clean, y, cv=group_cv, groups=X.argument_group, n_jobs=-1, method='predict_proba')[:,1]\n",
    "\n",
    "print(accuracy(y, gb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778798283262\n",
      "Wall time: 6.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=35, n_jobs=-1)\n",
    "scale_knn = StandardScaler()\n",
    "\n",
    "knn_pipe = Pipeline([(\"scale\", scale_knn), \n",
    "                     (\"model\", knn)])\n",
    "\n",
    "knn_predictions = cross_val_predict(knn_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, knn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729785407725\n",
      "Wall time: 5.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pt = Perceptron(random_state=42, n_jobs=-1)\n",
    "scale_pt = StandardScaler()\n",
    "\n",
    "pt_pipe = Pipeline([(\"scale\", scale_pt), \n",
    "                     (\"model\", pt)])\n",
    "\n",
    "pt_predictions = cross_val_predict(pt_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict')\n",
    "print(accuracy(y, pt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791416309013\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_params = {\n",
    "    'alpha': 0.0001,\n",
    "    'beta_1': 0.94,\n",
    "    'hidden_layer_sizes': (10, 6),\n",
    "    'learning_rate_init': 0.002}\n",
    "\n",
    "mlp = MLPClassifier(random_state=42, **best_params)\n",
    "\n",
    "scale_mlp = StandardScaler()\n",
    "\n",
    "mlp_pipe = Pipeline([(\"scale\", scale_mlp), \n",
    "                     (\"model\", mlp)])\n",
    "\n",
    "mlp_predictions = cross_val_predict(mlp_pipe, \n",
    "                                    X_clean, \n",
    "                                    y, \n",
    "                                    cv=group_cv, \n",
    "                                    groups=X.argument_group, \n",
    "                                    n_jobs=-1, \n",
    "                                    method='predict_proba')[:,1]\n",
    "print(accuracy(y, mlp_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer one ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ens_1 = pd.DataFrame()\n",
    "X_ens_1[\"rf\"] = rf_predictions\n",
    "X_ens_1[\"lr\"] = lr_predictions\n",
    "X_ens_1[\"gb\"] = gb_predictions\n",
    "X_ens_1[\"knn\"] = knn_predictions\n",
    "X_ens_1[\"pt\"] = pt_predictions\n",
    "X_ens_1[\"mlp\"] = mlp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805321888412\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr = LogisticRegression(C=0.5, random_state=42)\n",
    "\n",
    "lr_ens_1_predictions = cross_val_predict(lr, \n",
    "                                   X_ens_1, \n",
    "                                   y, \n",
    "                                   cv=group_cv, \n",
    "                                   groups=X.argument_group, \n",
    "                                   n_jobs=-1, \n",
    "                                   method='predict_proba')[:,1]\n",
    "print(accuracy(y, lr_ens_1_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf\n",
      "0.802660944206\n",
      "lr     2.283983\n",
      "gb     2.685078\n",
      "knn   -0.392785\n",
      "pt     0.122376\n",
      "mlp    1.145022\n",
      "dtype: float64\n",
      "\n",
      "lr\n",
      "0.802145922747\n",
      "rf     2.712612\n",
      "gb     1.151931\n",
      "knn    0.176760\n",
      "pt     0.133442\n",
      "mlp    1.469625\n",
      "dtype: float64\n",
      "\n",
      "gb\n",
      "0.804978540773\n",
      "rf     3.225946\n",
      "lr     2.487987\n",
      "knn   -0.710722\n",
      "pt     0.092963\n",
      "mlp    0.939386\n",
      "dtype: float64\n",
      "\n",
      "knn\n",
      "0.803862660944\n",
      "rf     2.657464\n",
      "lr     1.952294\n",
      "gb     0.395590\n",
      "pt     0.084557\n",
      "mlp    0.920477\n",
      "dtype: float64\n",
      "\n",
      "pt\n",
      "0.805236051502\n",
      "rf     2.879334\n",
      "lr     2.430326\n",
      "gb     0.548063\n",
      "knn   -0.747601\n",
      "mlp    0.923338\n",
      "dtype: float64\n",
      "\n",
      "mlp\n",
      "0.803605150215\n",
      "rf     3.102164\n",
      "lr     2.951968\n",
      "gb     0.779749\n",
      "knn   -0.794920\n",
      "pt     0.118503\n",
      "dtype: float64\n",
      "\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for model_to_drop in X_ens_1.columns:\n",
    "    print(model_to_drop)\n",
    "    X_ens_1_temp = X_ens_1.copy()\n",
    "    X_ens_1_temp = X_ens_1_temp.drop([model_to_drop], axis=1)\n",
    "\n",
    "    lr = LogisticRegression(C=0.5, random_state=42)\n",
    "\n",
    "    lr_ens_1_predictions = cross_val_predict(lr, \n",
    "                                       X_ens_1_temp, \n",
    "                                       y, \n",
    "                                       cv=group_cv, \n",
    "                                       groups=X.argument_group, \n",
    "                                       n_jobs=-1, \n",
    "                                       method='predict_proba')[:,1]\n",
    "    print(accuracy(y, lr_ens_1_predictions))\n",
    "    lr.fit(X_ens_1_temp, y)\n",
    "    print(pd.Series(lr.coef_[0], index=list(X_ens_1_temp.columns)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
